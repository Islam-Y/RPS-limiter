Техническое задание на разработку сервиса A (генератор HTTP-нагрузки)
0. Контекст платформы
   Тестовая платформа реализована в виде нескольких взаимодействующих сервисов, развернутых в Docker-контейнерах и связанных между собой по сети.
   Основа прототипа - три микросервиса на Java (Spring Boot):
   Сервис A (клиент-генератор нагрузки): имитирует множество клиентов и генерирует HTTP-запросы заданной интенсивности и профиля. Сервис воспроизводит разные типы нагрузочных сценариев (равномерный поток, всплески, аномальное поведение и т.д.) путем настройки частоты и паттерна отправки запросов. Все запросы сервиса A направляются в сервис C (limiterUrl — URL прокси-лимитера), который затем проксирует их в сервис B.
   Сервис B (целевой сервис): защищаемое приложение, к которому направляются запросы. В прототипе он играет роль упрощенной бизнес-логики (возвращает фиктивный ответ), основная задача - подвергнуться нагрузке и предоставлять метрики производительности (время отклика, процент успешных/ошибочных запросов). Сервис B служит индикатором того, как лимитирование защищает приложение: его стабильность под нагрузкой будет оцениваться.
   Сервис C (прокси-сервис с механизмом лимитирования): промежуточный прокси между A и B. Он принимает входящие запросы от генератора (A) и решает, пропустить запрос к сервису B или отклонить его, исходя из политики ограничения частоты. Сервис C реализует три алгоритма лимитирования: фиксированное окно, скользящее окно и бакет токенов. Выбор алгоритма и его параметры (например, размер окна или емкость/скорость пополнения бакета) могут переключаться в ходе экспериментов. Для хранения счетчиков запросов и токенов используется внешнее хранилище Redis. Redis развернут в отдельном контейнере; при сбое связи с Redis ограничитель переводится в режим fail-open и перестает блокировать запросы, чтобы не нарушить доступность системы.

   Помимо основных сервисов, в прототип включены инструменты мониторинга и интеллектуальный модуль:
   Система мониторинга (Prometheus): каждый из сервисов A, B, C экспонирует метрики через Spring Boot Actuator (время отклика, количество обработанных и отклоненных запросов, состояние очередей - при наличии и т.д.). Эти данные собираются Prometheus и используются для анализа результатов экспериментов.
  Интеллектуальный компонент: отдельный сервис на Python, прогнозирующий входящий трафик и выдающий рекомендации по настройке параметров лимитирования в режиме реального времени. Модуль использует модель Prophet для анализа истории запросов и выдачи прогнозов. Сервис C периодически отправляет в интеллектуальный модуль данные о трафике, после чего получает рекомендации; параметры алгоритма могут изменяться "на лету", делая политику ограничения адаптивной.

   Сценарии нагрузки эксперимента:
   Равномерная нормальная нагрузка: сервис A генерирует стабильный поток запросов с постоянной скоростью, не превышающей лимит. Система не должна отклонять запросы без необходимости, время отклика должно оставаться низким.
   Кратковременные всплески трафика: нагрузочный профиль включает пики, превышающие лимит в 2-3 раза, чередуемые с нормальными интервалами. Проверяется, как алгоритмы справляются с краткими перегрузками, и помогает ли интеллектуальный компонент заранее скорректировать лимиты.
   Превышение порога и аномальное поведение: имитация чрезмерной нагрузки (DDoS-подобный сценарий). Ожидается, что лимитирование эффективно отсечет избыточные запросы и защитит сервис B.
   Сбой хранилища или деградация узла: намеренное отключение или замедление Redis/компонентов. При fail-open лимитирование временно отключается, сервис B должен оставаться доступным насколько возможно, а система - восстановиться после возвращения Redis.

   Метрики оценки:
   Среднее время отклика (по ответам сервиса B).
   Процент отклоненных запросов (HTTP 429 Too Many Requests).
   Устойчивость при пиковых нагрузках (отсутствие падений B и скорость восстановления времени отклика).
   Уровень защиты от перегрузок (качественная оценка по совокупности метрик и логов).
1. Общий обзор и назначение
   Сервис A представляет собой клиент-генератор нагрузки, разработанный на Java с использованием фреймворка Spring Boot. Он предназначен для имитации различных профилей HTTP-трафика к целевому веб-сервису с целью исследования влияния алгоритмов ограничения запросов на устойчивость и защищённость распределённых систем (в контексте дипломного проекта). Сервис будет использоваться для нагрузки на систему под тестом, генерируя HTTP-запросы согласно заданному шаблону (профилю) нагрузки.
   Развёртывание: Сервис должен запускаться в Docker-контейнере для удобства развёртывания и изоляции окружения. Это означает, что итоговое приложение Spring Boot будет упаковано в Docker-образ вместе со всеми зависимостями, позволяя запускать сервис на любой платформе, поддерживающей Docker. По‑запросное планирование (как IntervalScheduler) для постоянного/синусоидального/всплесков — равномерность выше, микроберстов нет.
   Основные возможности:
   Гибкая конфигурация теста: Параметры нагрузки задаются во внешнем JSON-файле конфигурации. Этот файл определяет тип профиля нагрузки (паттерн генерации запросов), limiterUrl (URL прокси-лимитера) для нагрузки, длительность теста и другие настройки генерации трафика.


Различные профили нагрузки: Сервис поддерживает несколько типов трафик-паттернов (профилей нагрузки) для эмуляции разных сценариев:


Равномерная нагрузка (constant RPS): постоянная интенсивность запросов (запросы генерируются с постоянной частотой).


Всплески (burst): периодические пики нагрузки на фоне более низкого базового уровня (имитация всплесков трафика, например, флэшмобы или разовые акции).


Синусоидальная нагрузка: частота запросов плавно колеблется по синусоидальному закону (медленные периодические подъемы и спады интенсивности, что отражает суточные циклы или аналогичные паттерны трафика).


Пуассоновский поток (Poisson process): случайный поток запросов, имитирующий поведение большого количества независимых пользователей; интервалы между запросами распределены экспоненциально при заданной средней интенсивности (λ).


DDoS-подобная нагрузка: интенсивные, хаотичные всплески трафика, чередующиеся с относительно тихими периодами. Имитация атак типа "отказ в обслуживании", когда на целевой сервер обрушивается большое количество запросов в случайные моменты времени. Такой профиль включает непредсказуемые короткие «удары» высокой нагрузки с нерегулярными интервалами (hit-and-run стратегия).


Управление тестом: Предусмотрен интерфейс запуска и остановки нагрузки. Это может быть реализовано через REST API.
Метрики производительности: Во время генерации нагрузки сервис собирает и экспонирует ключевые метрики через Endpoint Prometheus (HTTP). В частности, доступны метрики времени отклика целей, количества успешно выполненных запросов и ошибок, а также текущей фактической интенсивности запросов (RPS) в реальном времени. Метрики публикуются в формате, совместимом с Prometheus, например через стандартный endpoint /actuator/prometheus Spring Boot.


Логирование: Сервис ведёт журнал событий и результатов в консоль (stdout). По возможности должна быть реализована опция логирования в файл (ротация логов может быть настроена средствами Docker или Linux). Логи позволят отслеживать процесс генерации нагрузки, в том числе запуск/остановку тестов, параметры текущего теста, суммарные показатели и возможные ошибки.


Масштабируемость: Решение должно быть спроектировано так, чтобы можно было запустить несколько экземпляров клиента генератора параллельно (горизонтально масштабировать генерацию нагрузки). Например, для имитации очень высокой нагрузки или распределённой атаки могут одновременно работать несколько контейнеров с сервисом A, каждый с собственной конфигурацией или нацеленный на разные узлы системы. Сервис A не хранит состояния между запросами (stateless), поэтому масштабирование достигается простым увеличением числа запущенных экземпляров.


Отсутствие бизнес-логики: Сервис не выполняет бизнес-операций и не обрабатывает сложные ответы – его единственная задача заключается в генерации HTTP-трафика с заданной частотой и шаблоном. Он должен быть максимально лёгким и производительным, чтобы сам не становился узким местом: по сути это нагрузочный клиент. Все получаемые ответы можно игнорировать или лишь учитывать для метрик (коды ответов и время задержки).


2. Формат конфигурационного JSON-файла
   Для запуска теста сервис принимает на вход JSON-файл с параметрами. Ниже описаны ключевые поля этого файла и их назначение. Все параметры можно задать через JSON; при запуске через REST API содержимое JSON может передаваться в теле запроса.
   Основные поля конфигурации теста:
   limiterUrl (строка): URL сервиса C (прокси-лимитер), на который отправляются HTTP-запросы генератором нагрузки. URL включает протокол, хост и путь (например: "http://service-c:8082/api/test"). Поддерживаются HTTP и HTTPS адреса. Метод запроса фиксирован - GET.


duration (число): длительность теста нагрузки в секундах. По истечении этого времени сервис автоматически прекратит посылать запросы. Например, 60 означает 60 секунд.


profile (объект): настройки профиля нагрузки. Этот объект определяет тип профиля и связанные параметры. Внутри него должны быть следующие поля:


type (строка): тип профиля нагрузки, один из поддерживаемых – "constant", "burst", "sinusoidal", "poisson", "ddos" (соответствуют перечисленным выше типам нагрузочных паттернов).


params (объект): вложенные параметры, зависящие от выбранного типа профиля. Ниже описано, какие параметры ожидаются для каждого типа:


Если type = "constant": профиль постоянной равномерной нагрузки. Параметры:


rps (число) – целевое постоянное количество запросов в секунду. Сервис будет стремиться равномерно выполнять примерно rps запросов каждую секунду на протяжении всего теста (идеально распределяя их по времени). Например, при rps: 50 – ~50 запросов каждую секунду (интервал между запросами около 20 мс постоянно). Такой профиль полезен для проверки системы в состоянии устойчивой нагрузки.


Если type = "burst": профиль периодических всплесков. Параметры:


baseRps (число) – базовая скорость запросов в секунду в фоновом режиме (между всплесками). Это уровень нагрузки в спокойные периоды.


spikeRps (число) – пиковая скорость во время всплеска нагрузки (количество запросов в секунду на пике).


spikeDuration (число) – длительность одного всплеска в секундах.


spikePeriod (число) – период всплесков в секундах (интервал от начала одного всплеска до начала следующего). Например, при spikePeriod: 60 и spikeDuration: 10 всплески происходят каждую минуту и длятся по 10 секунд. В течение всплеска отправляется spikeRps запросов в секунду, а вне всплесков – baseRps запросов/сек. Этот профиль позволяет проверить систему на способность выдерживать регулярные пиковые нагрузки. Пример: baseRps: 100, spikeRps: 500, spikeDuration: 5, spikePeriod: 60 – генератор будет обычно слать 100 RPS, но каждые 60 секунд в течение 5 секунд нагрузка возрастает до 500 RPS, затем возвращается к базе. (Такой паттерн моделирует, например, периодические всплески пользователей, заходящих волнами). Примечание: параметры можно расширить, например, позволить указывать нестабильность периода или амплитуды, но базовая версия предполагает фиксированные равномерные циклы.


Если type = "sinusoidal": профиль плавно изменяющейся (синусоидальной) нагрузки. Параметры:


minRps (число) – минимальная скорость запросов (нижний предел колебания).


maxRps (число) – максимальная скорость запросов (верхний предел колебания).


period (число) – период колебания в секундах (полный цикл синусоиды).


Сервис будет изменять интенсивность нагрузки по синусоидальному закону между minRps и maxRps. Формально, частота запросов как функция времени t может быть описана как:

RPS(t) = (minRps + maxRps) / 2 + (maxRps - minRps) / 2 * sin(2πt / period),

т.е. среднее значение равно средней точке между min и max, а амплитуда – половина разницы. Например, при minRps=50, maxRps=150, period=120 сгенерируется колеблющаяся нагрузка от 50 до 150 RPS с циклом 2 минуты. Такой профиль имитирует суточные или периодические колебания трафика (ночной спад и дневной пик и т.п.), часто наблюдаемые в реальных системах.


Если type = "poisson": профиль пуассоновского трафика (случайные независимые запросы). Параметры:


averageRps (число) – средняя интенсивность запросов (λ, в запросах в секунду).


Генератор будет слать запросы с случайными интервалами, распределёнными экспоненциально так, чтобы в среднем за секунду получалось averageRps запросов. Данный профиль опирается на пуассоновский процесс: времена между запросами генерируются по экспоненциальному закону с параметром λ = averageRps. На практике это означает, что в некоторые секунды может быть больше запросов, в другие меньше, но в среднем за длительный период интенсивность стремится к λ. Такой режим близко моделирует поведение большого числа независимых пользователей, каждый из которых делает запросы случайным образом, – в результате совокупный поток имеет статистические свойства пуассоновского. Например, averageRps: 20 – в среднем 20 запросов в секунду, но их распределение по времени нерегулярно (возможны кластеры запросов и паузы).


Если type = "ddos": профиль DDoS-подобной нагрузки (интенсивные хаотичные атаки). Параметры:


minRps (число) – фоновые запросы в период относительного затишья (может быть 0 или небольшой уровень фонового трафика).


maxRps (число) – максимальная интенсивность запросов во время атаки (пиковый уровень).


maxSpikeDuration (число) – максимальная длительность атакующего всплеска, секунд.


minIdleTime (число) – минимальный промежуток спокойного времени между атаками, секунд.


maxIdleTime (число) – максимальный промежуток спокойного времени между атаками, секунд.


Данный профиль генерирует непредсказуемые всплески нагрузки: периоды очень высокой интенсивности (до maxRps RPS) длящиеся короткое время (случайно до maxSpikeDuration), чередуются со случайными интервалами затишья (длительностью между minIdleTime и maxIdleTime, в которые нагрузка либо отсутствует, либо равна minRps). Идея – смоделировать поведение распределённой атаки, когда злоумышленники посылают трафик порциями, стараясь сбить защиту. В частности, можно смоделировать burst-атаки (так называемые "hit-and-run"): серия коротких ударов по несколько секунд с огромным трафиком, разделённых минутами или часами простоя. Пример: minRps: 0, maxRps: 1000, maxSpikeDuration: 10, minIdleTime: 20, maxIdleTime: 60 – атаки длятся от нескольких до 10 секунд с интенсивностью до 1000 RPS, потом пауза 20–60 секунд, затем снова атака и т.д.


Примечание: Поля profile.type и соответствующие params обязательны. Если указаны лишние параметры, несоответствующие выбранному профилю, они могут игнорироваться (или вызывать ошибку конфигурации, в зависимости от реализации). В будущем можно рассмотреть поддержку комбинированных или более сложных профилей, но базовая версия ограничивается одним типом за тест.
Другие возможные поля (опционально, для расширения конфигурации):
requestsPerThread / concurrency: число параллельных потоков или асинхронных запросов, которые генератор будет использовать. В базовой реализации это поле можно опустить – сервис сам определит оптимальное число потоков/асинхроных задач исходя из требуемого RPS. Однако, при желании, можно указать явно, например, concurrency: 10 для использования 10 параллельных рабочих потоков.


requestOptions: объект с настройками HTTP-запроса (если потребуется настраивать заголовки, параметры запроса, аутентификацию и пр. – см. раздел Возможные расширения). В базовой версии метод фиксирован GET, тело запроса не используется.


3. Интерфейс запуска и управления тестом
   Сервис предоставляет возможность управлять генерацией нагрузки через REST API. Оба способа являются альтернативными; выбор зависит от сценария использования. Ниже описана структура обоих вариантов:
   3.1 REST API
   При развёртывании сервиса как постоянно работающего микросервиса, удобно организовать REST API для управления. Базовый адрес сервиса A далее именуется loadGeneratorUrl (например, http://service-a:8080). Предлагается реализовать следующие эндпоинты (URI и типы запросов могут быть скорректированы при реализации):
   POST /test/start – запуск теста нагрузки.


Входные данные: тело запроса – JSON с конфигурацией теста (в том же формате, как описано выше, либо путь/имя заранее загруженного конфигурационного профиля).


Действие: Сервис читает параметры, инициализирует генератор нагрузки согласно профилю, и начинает отправлять запросы.


Выходные данные: возвращает подтверждение запуска, например JSON с {"status":"started", "testId": "<идентификатор>"}, где testId – уникальный ID запущенного теста. Если уже запущен другой тест, можно вернуть ошибку (409 Conflict) или опционально остановить предыдущий и запустить новый (но одновременное выполнение двух тестов в одном экземпляре, как правило, не предполагается).


POST /test/stop – остановка текущего теста.


Входные данные: можно не передавать ничего (либо идентификатор теста, если возможно несколько, но базовая реализация – один тест).


Действие: Сервис останавливает генерацию нагрузки заранее, до истечения запланированной длительности. Все рабочие потоки/таски прекращают отправлять запросы по целевому URL.


Выходные данные: подтверждение остановки {"status":"stopped", "testId":"..."}. Если никакой тест не выполнялся – возвращается ошибка (404 Not Found или 400 Bad Request).


GET /test/status – получение статуса текущего теста.


Выходные данные: JSON с информацией о том, выполняется ли сейчас нагрузочный тест, и если да, то его параметры и прогресс. Например:

{
"running": true,
"testId": "12345",
"profile": "burst",
"elapsedTime": 34,
"duration": 60,
"requestsSent": 3400,
"errors": 0
}
Если тест не запущен: {"running": false}. Также статус можно расширить, включая текущую среднюю RPS, последние замеры и т.п.


(Опционально) GET /test/config – отдавать текущую конфигурацию, если тест идёт.


Замечания по REST:
Формат и коды ответов должны быть задокументированы. Например, если передан некорректный JSON – возвращать 400 с сообщением об ошибке парсинга; если попытка запуска при уже запущенном тесте – 409 Conflict, и т.д.


Реализация: можно использовать Spring Web (Spring MVC или WebFlux) контроллеры для этих эндпоинтов. Запуск теста будет, вероятно, выполняться асинхронно (не блокировать сам HTTP запрос управления). Т.е. POST /start быстро вернёт ответ, а генерация нагрузки пойдёт в фоновых потоках.
4. Экспортируемые метрики (интеграция с Prometheus)
   Для мониторинга работы генератора и измерения производительности целевого сервиса, сервис A экспонирует ряд метрик в формате Prometheus. Метрики доступны по HTTP endpoint-у (например, GET /actuator/prometheus при включённом Spring Boot Actuator с зависимостью micrometer-prometheus). Эти метрики можно скрапить Prometheus-сервером с заданным интервалом, а затем визуализировать (например, Grafana) или использовать для оценки эффективности алгоритмов rate limiting.
   Основные метрики:
   loadgen_requests_total – счётчик (Counter), общее количество HTTP-запросов, отправленных генератором с момента старта теста. Имеет метки (labels):


status: статус результата запросов, например "success" для 2xx ответов, "rate_limited" для 429 Too Many Requests и "error" для остальных ошибок (включая 4xx/5xx, кроме 429, и сетевые ошибки). Код 429 означает, что клиент превысил установленный лимит запросов за определенный промежуток времени. Таким образом, из одной метрики можно получить общее число запросов, долю успешных и долю отклоненных лимитером.


(возможно) target: метка с хостом или именем целевого сервиса (полезно, если один генератор может работать по разным целям; в простейшем случае можно опустить).


Назначение: позволяет строить графики RPS (по приросту счётчика в единицу времени), вычислять долю ошибок и долю отклоненных лимитером запросов. Counter в Prometheus мономно возрастает в течение работы приложения и сбрасывается при перезапуске.


loadgen_request_duration_seconds – гистограмма (Histogram) или суммарный таймер (Summary), измеряющий время отклика HTTP-запросов (latency) в секундах. Каждый запрос при получении ответа (или таймаута) фиксирует свою длительность.


Если используется гистограмма, она определяется набором бакетов (например, [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5] секунд) и экспонирует ряд метрик: суммарное время, количество, и разбивку по бакетам (стандартный формат Prometheus для Histogram).


Если используется Summary, то экспонируются фрагментиль (квантили) времени ответа. Но предпочтительно Histogram, так как с ней легче вычислять агрегированные показатели по всем генераторам.


Назначение: позволяет наблюдать распределение задержек на сервере под нагрузкой – например, среднее время ответа, percentiles (p90, p99) и т.д., что важно для анализа производительности системы под разными профилями нагрузки.


loadgen_current_rps – гейдж (Gauge), показывающий текущую скорость генерации запросов (в запросах в секунду) в данный момент. Этот показатель можно обновлять либо каждую секунду (например, вычисляя сколько запросов было за последнюю секунду), либо получать непосредственно из внутреннего планировщика запросов. Gauge будет динамически расти или падать в соответствии с профилем: для постоянной нагрузки он стабилен около заданного значения, для синусоидальной – колеблется, для всплесков – покажет пики во время всплесков.


Примечание: хотя RPS можно рассчитать в Prometheus на основе производной от requests_total, прямой gauge удобен для мгновенного отслеживания.


loadgen_errors_total – (опционально) отдельный Counter для количества ошибок (альтернатива метке status в requests_total). Это может быть удобно, если хочется напрямую график ошибок. Однако дублировать не обязательно – либо метка, либо отдельный счётчик.


loadgen_active_threads – (опционально) Gauge, показывающий число потоков/корутин, в данный момент выполняющих запросы. Полезно, если генерация реализована многопоточно, чтобы видеть, сколько параллельных запросов происходит.


loadgen_test_running – Gauge (или флаг) со значением 1, когда тест запущен, и 0, когда нет (чтобы Prometheus мог зафиксировать момент старта/окончания теста; хотя можно определять по изменениям в других метриках).


Системные метрики Java – Поскольку используется Spring Boot Actuator Micrometer, по умолчанию доступны множество JVM и системных метрик (память, CPU, потоки и т.п. – они тоже будут видны на /actuator/prometheus). Они не специфичны для сервиса A, но могут быть полезны, например, чтобы убедиться, что сам генератор не перегружен (например, смотреть его потребление CPU или паузы GC).


Например, jvm_memory_used_bytes, process_cpu_usage, jvm_threads_live и т.д., а также http.server.requests (статистика по самим эндпоинтам REST управления сервисом, хотя они редки). В контексте диплома основное внимание на пользовательские метрики выше, но упомянуть эти дефолтные стоит.


Экспонирование метрик: С сервисом будет интегрирован Prometheus-клиент (например, Micrometer с PrometheusRegistry). При запуске Spring Boot приложения и включении Actuator’s Prometheus endpoint, все метрики доступны по пути /actuator/prometheus. Необходимо убедиться, что в application.properties включено:
management.endpoints.web.exposure.include=health,prometheus
management.endpoint.prometheus.enabled=true

Таким образом, Prometheus сможет опрашивать контейнер генератора (достаточно добавить в конфиг Prometheus джобу, указывающую на хост и порт сервиса A).
Пример содержимого /actuator/prometheus:
# HELP loadgen_requests_total Total HTTP requests sent by load generator
# TYPE loadgen_requests_total counter
loadgen_requests_total{status="success"} 12345
loadgen_requests_total{status="rate_limited"} 89
loadgen_requests_total{status="error"} 67

# HELP loadgen_request_duration_seconds Histogram of request durations
# TYPE loadgen_request_duration_seconds histogram
loadgen_request_duration_seconds_bucket{le="0.05"} 8000
...
loadgen_request_duration_seconds_bucket{le="+Inf"} 13000
loadgen_request_duration_seconds_sum 120.34
loadgen_request_duration_seconds_count 13000

# HELP loadgen_current_rps Current requests per second rate
# TYPE loadgen_current_rps gauge
loadgen_current_rps 250.0

# HELP loadgen_active_threads Number of active request threads
# TYPE loadgen_active_threads gauge
loadgen_active_threads 12

(И так далее, плюс стандартные метрики JVM.)
Эти данные затем могут быть проанализированы для получения результатов теста: например, максимальная достигнутая RPS, процент ошибок, средний и 95-й перцентиль времени ответа и т.п.
5. Внутренняя архитектура сервиса
   Архитектура сервиса A построена модульно, разделяя функциональные компоненты: конфигурация, генерация нагрузки, профили нагрузки, сбор метрик и т.д. Ниже перечислены основные компоненты и их роль:
   Модуль конфигурации – отвечает за чтение и парсинг входного JSON-файла. При запуске через REST – принимает JSON из тела запроса. Этот модуль валидирует конфигурацию (например, проверяет, что указаны все необходимые поля, значения в допустимых диапазонах). Результатом работы является объект (например, TestConfig), содержащий структуру всех настроек, включая тип профиля и его параметры.


Компонент профиля нагрузки (Profile Logic) – реализует логику конкретного профиля нагрузки. Для каждого типа профиля может быть своя стратегия/класс:


Например, класс ConstantLoadProfile с методом, выдающим интервал между запросами или напрямую управляющим отправкой с фиксированным RPS.


Класс BurstLoadProfile – содержащий таймеры или логические счётчики времени, когда должен начаться следующий всплеск и т.д.


SinusoidalProfile – вычисляет в текущий момент требуемую интенсивность по формуле синуса.


PoissonProfile – использует генератор случайных чисел (экспоненциальное распределение) для определения пауз между запросами.


DdosProfile – генерирует случайные пики: может быть реализован, например, через планирование "атаки" (внезапного увеличения RPS) с помощью случайных таймеров.


Эти классы инкапсулируют алгоритм определения когда и сколько запросов нужно слать в единицу времени. Они могут предоставлять интерфейс, например, метод nextInterval() – сколько ждать до следующего запроса, или метод currentRps(t) – вычислить желаемый RPS в момент времени t.


Планировщик запросов (Scheduler) – сердцевина генератора. Он отвечает за режим отправки HTTP-запросов с нужной скоростью. Возможны два подхода:


На основе интервалов: Планировщик запрашивает у компонента профиля время до следующего запроса (например, nextInterval()). Затем спит/ждет указанное время, после чего запускает новый HTTP-запрос, снова получает следующий интервал и т.д. (В многопоточной реализации может быть пул потоков, каждый из которых занимается циклом "ждать-интервал -> послать запрос").


На основе заданий в секунду: Например, можно каждую секунду планировать определённое число задач-HTTP-запросов согласно требуемому RPS. Для синусоиды/пуаcсона – RPS меняется, но можно пересчитывать каждый секунду.


Планировщик должен быть способен точно соблюсти профиль (насколько возможно в условиях планирования задач в ОС/JVM). В Java можно использовать ScheduledExecutorService, Timer, или для более высокой точности – реактивный подход (Project Reactor, WebFlux, где можно контролировать поток запросов).

Многопоточность и асинхронность: Планировщик также управляет пулом потоков или асинхронными воркерами, чтобы посылать несколько запросов параллельно, особенно при высоком RPS. Например, может быть выделен пул из N потоков, или использовать реактивный WebClient (Netty) для большего параллелизма без взрывного роста потоков.


HTTP-клиент (Request Sender) – модуль, непосредственно выполняющий HTTP-запросы к целевому limiterUrl. Он может быть реализован с помощью:


Библиотеки Spring WebClient (реактивный, не блокирующий) – предпочтительно для очень больших нагрузок, чтобы эффективно использовать ресурсы.


Или RestTemplate/HttpClient с пулом соединений – для более простого подхода (но он блокирующий, значит на каждый параллельный запрос нужен поток).


Клиент должен поддерживать базовые вещи: возможно, переиспользование keep-alive соединений, настройку таймаута на запрос (чтобы не зависать слишком долго на одном запросе), опционально поддержку HTTPS (что, впрочем, обычно работает из коробки).


Ограничение скорости: Хотя генерация профиля задаёт частоту, нужно учитывать сетевые условия; в идеале, использовать неблокирующую отправку и не дожидаться завершения предыдущих запросов, иначе высокую RPS не получить. Поэтому, например, WebClient с .flatMap() для параллельных запросов.


Обработка ответа: Содержимое ответа, как правило, не важно – но HTTP-код важно учитывать (для метрик успех/ошибка). Поэтому после выполнения запроса нужно проверить статус: если не в диапазоне 200-299, считать как ошибку; 429 учитывать отдельно как rate limited. Время выполнения запроса = разница между отправкой и получением ответа (или таймаута).


Если запрос не получил ответ (timeout или соединение отклонено), тоже считать как ошибка (возможно, с отдельной меткой причины).


Модуль сбора метрик – отвечает за отслеживание результатов запросов и обновление метрик Prometheus. Это можно сделать несколькими способами:


Использовать Micrometer: например, завести Counter requestsTotal и вызывать .increment() на нём для успеха или ошибки (с метками), Timer для времени ответа (Timer.record(duration)), Gauge для RPS (Micrometer позволяет определять Gauge функцией, или вручную обновлять через Gauge.register).


Либо вручную вести счётчики и отдавать их через отдельный endpoint. Но раз Spring Boot Actuator включен, проще интегрироваться в него и использовать /actuator/prometheus.


Процесс: когда HTTP-клиент получает результат запроса, он сообщает модулю метрик, например вызывая metrics.recordRequest(Duration d, boolean success). Модуль метрик внутри инкрементирует нужные счетчики/гистограммы. Важно: Метрики должны быть потокобезопасны, т.к. многие потоки могут одновременно записывать. Micrometer abstractions обычно потокобезопасны.


Test Manager: можно выделить класс, управляющий состоянием теста – запускает планировщик в отдельном потоке, хранит флаг выполнения, обеспечивает возможность остановки. Например, при старте сохраняется Future или thread, который выполняет нагрузку, и при вызове /stop этот поток прерывается или задаётся флаг завершения.


Контроллер статуса опрашивает этот менеджер: сколько запросов выполнено, сколько времени прошло и т.д. (Такие данные дублируют метрики, но можно и напрямую хранить счётчики в менеджере).


Остановка теста: должна быть аккуратно реализована – например, через volatile boolean флаг running = false + у планировщика проверка этого флага между запросами (или прерывание потоков). После остановки все ресурсы (потоки, соединения) освобождаются.


Логирование (Logging) – кросс-компонентный аспект: все важные события пишутся в лог. Используется стандартный логгер (например, SLF4J + Logback).


На info уровне: запуск теста (с параметрами), завершение теста (с итогами: сколько запросов, ошибок, ср. время ответа и т.п.), остановка по команде, существенные изменения (например, начало всплеска, если нужно).


На error уровне: исключения, сбои при отправке запросов, например невозможность резолва DNS, соединение отказано, и т.д. (впрочем, такие вещи можно учитывать и как часть ошибки запроса).


Логи выводятся в консоль (stdout) – в Docker они будут собираться лог-драйвером Docker. По желанию настраивается также вывод в файл (например, logs/app.log внутри контейнера), но это не строго обязательно, можно опираться на консольные логи.


Формат логов: желательно включать временные метки, уровень, сообщение. Например: [2026-01-10 12:00:00,123 INFO] Started load test (profile=burst, limiterUrl=http://service-c:8082/api/test, duration=60, baseRps=100, spikeRps=500)...


Docker-окружение: Сервис должен корректно работать в контейнере. Это означает, что никаких привязок к локальной файловой системе (кроме как для конфигов, которые мапятся), порты должны быть настраиваемыми через переменные (например, SERVER_PORT), etc. В Dockerfile потребуется:


От базового образа копировать jar,


Определить ENTRYPOINT, который запускает приложение (java -jar ...),


Экспонировать порт (например 8080).



Для поддержки обоих вариантов, можно сделать так: если при запуске контейнера указан env CONFIG_FILE, то сразу запуск теста и выход, иначе – запуск веб-сервера. Но это усложняет. Возможно проще собрать два образа или запускать с разными аргументами. В рамках ТЗ можно описать один способ (REST-сервис), а CLI использовать вне Docker (или в Docker, но как показано выше, одноразово).


Модуль расширяемости (hook-ы): архитектура должна позволять в будущем добавлять новые типы профилей (например, Ramp-up, Random Walk и т.д.), новые опции (аутентификация, прокси) без значительных переделок. Для этого профили лучше сделать интерфейсом/абстрактным классом, а не жёстко зашитыми if-ами. Аналогично с HTTP-клиентом: использовать абстракцию (интерфейс RequestExecutor), чтобы его можно было заменить (например, на другой HTTP-бэкенд).


Диаграмма компонентной архитектуры может выглядеть следующим образом (логическая схема):
Input: JSON Config (через файл или REST) →


Config Parser (создаёт TestConfig объект) →


Profile Factory (по типу профиля из конфигурации создаёт инстанс ProfileStrategy) →


Test Manager (запускает тест: создает Scheduler и HTTP Client, инициирует Worker Threads) →


Scheduler (в цикле генерирует по ProfileStrategy либо напрямую RPS и выдаёт задания HTTP Client'у) →


HTTP Client (отправляет запросы, возвращает результат) →


Metrics (фиксирует результат, считает RPS, ошибки, время) + Logger (пишет события) →


... до остановки по времени или команде.


Prometheus Endpoint (параллельно доступен для опроса, предоставляется Micrometer).


(Изображение, если бы было, показало бы как конфиг идёт в профиль, профиль в планировщик, тот управляет worker-ами, они шлют запросы и отчитываются в метрики.)
6. Масштабируемость и производительность
   Сервис A изначально проектируется так, чтобы его можно было масштабировать и адаптировать под разную нагрузку:
   Горизонтальное масштабирование: Для увеличения общей генерируемой нагрузки можно запускать несколько экземпляров сервиса параллельно (например, на разных узлах или контейнерах). Они могут работать независимо, каждый на свой целевой URL или на один и тот же (если нужно скоординированно повысить RPS). Поскольку состояние теста полностью локально в пределах одного экземпляра и внешний мир видит их как отдельных клиентов, проблем с согласованностью нет. Если необходимо, координация (например, одновременный старт) может осуществляться внешним оркестратором. В итоге суммарная нагрузка складывается. Примечание: метрики Prometheus с разных инстансов будут раздельными, их можно агрегировать с помощью promQL при анализе (например, sum по instance).


Вертикальная масштабируемость: На уровне одного экземпляра сервис должен эффективно использовать доступные ресурсы:


Использование асинхронного I/O или достаточного числа потоков позволяет генерировать тысячи RPS с одной машины. Следует убедиться, что нет блокирующих операций, которые лимиитируют throughput (например, запись логов очень частая – можно делать не чаще чем X в секунду или писать агрегировано).


В Docker контейнере можно настроить лимиты CPU/Memory; сервис должен корректно работать и под ограничениями (вплоть до того, что если не хватает CPU на заданный RPS, реальный RPS будет ниже, но сервис остаётся стабильным).


Тестирование производительности генератора: Так как цель – нагрузить систему, сам генератор должен быть лёгким. Желательно провести профилирование или тесты генерации N запросов/с (к некоему локальному быстрым endpoint) чтобы удостовериться, что он способен достичь нужной интенсивности. Если возникнут узкие места (например, у RestTemplate – блокировка на сокетах, или у WebClient – слишком много контекста), можно тюнинговать (увеличить размеры пулов, оптимизировать JSON parse, отключить лишние Spring Boot компоненты, например, не грузить веб-сервер если не нужен).


Ограничения: Один экземпляр, конечно, не бесконечно масштабируется – например, на 1 vCPU генерировать более 1000-2000 RPS может быть уже проблематично (в зависимости от простоты целевого URL). Поэтому на очень высокие нагрузки (десятки тысяч RPS) точно потребуется несколько экземпляров и/или распределение по машинам.


Масштабируемость управления: Если предполагается одновременный запуск нескольких тестов, архитектура может быть расширена для поддержания списка активных тестов. Однако, по ТЗ параллельно один экземпляр запускает один тест. Для большего – использовать несколько экземпляров (или доработать под мульти-тест, но это сложно).


7. Логирование
   Как упоминалось, сервис ведёт протокол работы в логах. Ключевые аспекты логирования:
   Техническая реализация: Использовать встроенные возможности Spring Boot (логгер Logback по умолчанию) или другую библиотеку, но настроить её через application.properties (например, уровень логирования и шаблон вывода). По умолчанию Spring Boot пишет в консоль. Можно указать logging.file.name=app.log чтобы дублировать лог в файл.


Формат сообщений: Включает отметку времени, уровень, источник (класс) и текст. Например:

2026-01-10 15:45:12.345  INFO 1 --- [main] LoadGeneratorApp : Starting load test {profile=constant, rps=100, duration=60, limiterUrl=http://service-c:8082/api/test}
2026-01-10 15:45:13.789  WARN 1 --- [pool-1-thread-3] HttpClient : Request failed (Connection refused) - will count as error.
2026-01-10 15:46:12.001  INFO 1 --- [main] LoadGeneratorApp : Test completed: 6000 requests sent, 0 errors, avg latency 50ms, p95 80ms.


Уровни логирования:


INFO – основная информация о ходе теста: старт, параметры, окончание, основные статистики.


DEBUG – более подробная отладочная информация (по умолчанию выключено). Например, можно на debug выводить каждые N секунд промежуточный прогресс, или подробности планирования (но это не стоит делать на большом RPS, чтобы не замедлять).


ERROR – неожиданные ситуации: ошибки парсинга конфига, необработанные исключения в потоках генерации, и т.д.


WARN – предупреждения, например если слишком медленный отклик или пропуски запросов.


Консоль vs файл: В Docker среде обычно достаточно консоли, т.к. сбор логов централизован. Но можно настроить опциональный вывод в файл (например, через volume). Если реализуется – файл должен расти ограниченно (можно настроить rolling policy, например, раз в 10MB новый).


Примеры лог-сообщений:


При запуске: "Starting load test with profile=burst (baseRps=100, spikeRps=500) for 120 seconds targeting limiterUrl=http://service-c:8082/api/test..."


При достижении всплеска (если нужно): "Burst peak started at t=30s (target RPS=500)"


При ошибке запроса: "Request error: HTTP 500 Internal Server Error (counting as failure)".


При остановке: "Load test stopped by user after 45s. Sent 4500 requests."


Структурированность: Логировать в JSON-формате для удобства разбора, но текстовый читабельный формат тоже подходит.


_________________________________________________________________________


Техническое задание: Сервис B (целевой сервис)
Обзор
Сервис B представляет собой целевое (защищаемое) приложение, выполняющее упрощённую бизнес-логику. В контексте прототипа системы нагрузочного тестирования сервис B принимает HTTP-запросы, перенаправленные от прокси-лимитера (сервиса C), и возвращает фиктивный ответ. Основная задача сервиса B – подвергаться нагрузке и предоставлять метрики производительности (например, время отклика, процент успешных и ошибочных запросов). Эти метрики используются для оценки эффективности механизмов ограничения частоты запросов (Rate Limiting) и анализа того, насколько лимитирование защищает работоспособность приложения B под нагрузкой.
Назначение и роль сервиса B
Роль в системе: Сервис B играет роль условного «рабочего» приложения, которое защищается от перегрузок. Он служит индикатором устойчивости системы: по стабильности и производительности сервиса B под различной нагрузкой оценивается эффективность алгоритмов лимитирования в сервисе C.


Бизнес-логика: В прототипе бизнес-логика сервиса B упрощена до минимума – он, например, возвращает заранее сформированный фиктивный ответ (статические данные), имитируя успешную обработку запроса. Таким образом исключается влияние сложной логики, а основной упор делается на способность сервиса обрабатывать входящие запросы.


Нагрузочная роль: Сервис B должен корректно функционировать как под нормальной, так и под повышенной нагрузкой. Предполагается, что при перегрузке (когда частота запросов превышает возможности обработки) внешний лимитирующий сервис C отфильтрует избыточные запросы, защищая сервис B от деградации. Стабильность работы сервиса B в условиях экстремальной нагрузки будет ключевым показателем эффективности такой защиты.


Функциональные требования
Обработка HTTP-запросов: Сервис B должен предоставлять REST API эндпоинт GET /api/test, принимающий запросы от сервиса C. Каждый полученный запрос обрабатывается и завершается формированием ответа:


Ответ содержит фиктивные данные (например, простое сообщение или статичный JSON-объект), подтверждающие успешную обработку. Формат и содержание ответа фиксированы для упрощения (например, всегда возвращается статус 200 OK и тело с текстом "OK").


В случае внутренней ошибки сервис B возвращает соответствующий код (например, 500 Internal Server Error), чтобы метрики фиксировали процент ошибочных запросов.


Отсутствие встроенного лимитирования: Сервис B не должен самостоятельно отклонять или ограничивать входящие запросы по частоте. Все решения о пропуске либо отклонении трафика принимает внешний сервис C. Сервис B пытается обработать любой поступивший к нему запрос, демонстрируя реальную нагрузку на приложение при разных режимах работы лимитирования.


Параллельная обработка: Реализация сервиса B должна поддерживать многопоточную обработку, чтобы выдерживать concurrent-нагрузку. Необходимо настроить пул потоков веб-сервера (Spring Boot/Tomcat) на разумные пределы, обеспечивающие одновременно:


Эффективную обработку нескольких одновременных запросов.


Предотвращение истощения ресурсов системы при экстремальной перегрузке (например, с помощью ограничения максимального числа рабочих потоков и очереди запросов).


Фиктивная нагрузка (опционально): Для реалистичности можно предусмотреть имитацию небольшой задержки или вычислительной работы внутри обработчика запроса (например, Thread.sleep(5) мс или выполнение тривиальной операции). Это эмулирует время обработки бизнес-логики и позволяет получить метрики времени отклика, близкие к реальным.


Логирование: Сервис B логирует только ошибки или исключения, произошедшие при обработке, с стектрейсом. Для анализа нормального поведения используются метрики.


Интеграция с другими компонентами
Получение трафика через сервис C: Сервис B расположен за прокси-слоем лимитирования (сервис C). Клиенты (генератор нагрузки A) не обращаются напрямую к B – все запросы проходят через сервис C, который решает, какие запросы пропустить. Таким образом, сервис B получает только трафик, пропущенный политикой ограничения частоты. В конфигурации системы (Docker Compose или аналогичной) сервис C знает адрес/порт сервиса B и маршрутизирует допустимые запросы на него.


Взаимодействие с Redis (опосредованно): Сервис B напрямую не взаимодействует с хранилищем Redis. Вся работа с Redis (хранение счетчиков запросов, токенов для алгоритма «ведро токенов» и т.д.) реализована в сервисе C. Для сервиса B это прозрачно – он получает либо ограниченный поток запросов (при нормальной работе Redis), либо неограниченный поток (если лимитер перешёл в режим fail-open при сбое Redis).


Связь с интеллектуальным модулем: Сервис B никак не взаимодействует с внешним интеллектуальным модулем прогнозирования. Этот модуль через REST API общается с сервисом C и косвенно влияет на B тем, что изменяет параметры лимитирования (например, повышает или снижает порог запросов в секунду). В итоге до B может доходить разный объем трафика в зависимости от прогнозных корректировок, но сам B остаётся пассивным получателем этих изменений.


Мониторинг (Prometheus): Сервис B экспонирует свои метрики через HTTP-эндпоинт (см. раздел "Метрики и мониторинг"). Система мониторинга Prometheus настроена опрашивать (scrape) этот эндпоинт с заданным интервалом. Таким образом, сервис B интегрирован в общую систему мониторинга, предоставляя данные для анализа в реальном времени (например, в Grafana) и последующего сравнения результатов тестов.


Деплой и окружение: Сервис B развёртывается в отдельном Docker-контейнере. Взаимодействие с другими контейнерами включает:


Открытие порта сервиса B (8081) для доступа сервиса C.


Подключение к общей сети Docker, чтобы сервис C мог обращаться к B по имени хоста контейнера.


Переменные окружения или файлы конфигурации внутри контейнера, позволяющие настраивать параметры (порт, префиксы URL и др.) при запуске.


Метрики и мониторинг
Сервис B должен экспонировать ключевые метрики, которые система мониторинга будет собирать для оценки производительности. Реализация мониторинга опирается на Spring Boot Actuator и библиотеку Micrometer с Prometheus-registry.
Экспорт метрик: В приложении B необходимо включить Spring Boot Actuator с эндпоинтами health и prometheus. Это обычно достигается добавлением зависимостей spring-boot-starter-actuator и micrometer-registry-prometheus, а также настройкой в application.properties (например, management.endpoints.web.exposure.include=health,prometheus). В результате по URL, например http://<host>:8081/actuator/prometheus, сервис B будет выдавать стандартные метрики приложения в формате, пригодном для Prometheus.


Ключевые метрики сервиса B:


Время отклика (latency): распределение или среднее время обработки HTTP-запросов. Actuator/Micrometer автоматически предоставляет метрику, например, http_server_requests_seconds (в разбивке по endpoint/status). Это позволит отслеживать, как изменяется время ответа B при росте нагрузки.


Пропускная способность: количество запросов, обрабатываемых сервисом B, например метрика http_server_requests_seconds_count (общее число обработанных запросов) и ее производная – число запросов в секунду.


Успешные и ошибочные ответы: на основе метрик HTTP можно выделить процент успешных запросов (с кодами 2xx) и процент ошибок (5xx). Например, http_server_requests_seconds_count{status="200"} vs {status="500"} покажут соотношение. Это важно для сценариев перегрузки – идеальный лимитирующий алгоритм должен удерживать ошибки на стороне B близкими к нулю.


Ресурсы системы: при возможности, можно включить метрики JVM (память, загрузка CPU) и метрики пула потоков или очередей запросов. Например, размер активных потоков (если доступен через Actuator thread pool metrics) или длина очереди задач. Это поможет понять, насколько ресурсы сервиса B напрягаются под нагрузкой.


Мониторинг доступности: Actuator предоставляет эндпоинт здоровья /actuator/health. Его следует оставить включённым. Prometheus можно настроить на проверку здоровья, а оркестратор контейнеров – на рестарт при падении. Однако, так как цель тестов – наблюдать деградацию B под нагрузкой, автоматический рестарт при ухудшении лучше отключить, чтобы не скрыть эффектов перегрузки.


Сбор данных Prometheus: Prometheus настроен собирать метрики сервиса B (и других сервисов) с заданным интервалом (например, каждые 5-15 секунд). Убедитесь, что в конфигурации Prometheus (prometheus.yml) добавлен job для сервиса B с правильным адресом и портом. Эти данные будут использоваться для построения графиков и расчёта итоговых метрик эффективности после тестов.


Визуализация: В Grafana или аналогичной системе на основе собранных метрик будут построены дашборды, например: график времени отклика B по сценариям, график числа запросов в секунду, доли ошибок. Это необходимо для наглядной демонстрации результатов экспериментов по каждому алгоритму лимитирования.


Нефункциональные требования
Производительность и задержки: В нормальном режиме (нагрузка ≤ лимита) сервис B должен обеспечивать минимально возможное время отклика (порядка нескольких миллисекунд для фиктивной логики) и высокую пропускную способность, ограниченную лишь заданным лимитом. Рост времени отклика под нагрузкой будет сигнализировать о приближении к пределам пропускной способности. Требование: при сценарии равномерной нагрузки, не превышающей лимит, среднее время ответа B должно оставаться стабильным и низким, без ошибок.


Поведение под перегрузкой: В ситуациях, когда входящая нагрузка превышает лимиты (например, флэшмоб или атака), ожидается, что благодаря сервису C до B дойдёт ограниченный поток запросов. Сервис B должен выдерживать поток на уровне заданного лимита без существенной деградации (возможно некоторое увеличение времени отклика, но без массовых отказов). Если же лимитирование отключено (сценарий отказа), сервис B должен максимально долго оставаться работоспособным под лавиной запросов:


Допустимо увеличение очередей и времени отклика, возможно некоторое количество ошибок 5xx из-за таймаутов или истощения ресурсов, но сервис не должен полностью упасть (крайний случай, которого следует избегать).


После прохождения пика нагрузку и восстановления нормального режима, сервис B должен вернуться к стабильной работе (время отклика снижается до исходных значений, ошибки исчезают).


Отказоустойчивость: Архитектура сервиса B изначально без состояния (stateless); в рамках прототипа используется один экземпляр сервиса. Также, реализованная стратегия fail-open (в сервисе C) подразумевает, что лучше временно перегрузить B, чем полностью остановить сервис для клиентов – B должен это учитывать. В случае сбоя внешних компонентов (Redis, C) B продолжает работу автономно, обслуживая всё, что к нему приходит.


Удобство наблюдения и анализа: Так как ключевая цель сервиса B – предоставить данные для анализа, необходимо обеспечить, чтобы все важные события и состояния фиксировались либо метриками, либо логами. Например, если сервис B начнёт тормозить и ставить запросы в очередь, хорошо, чтобы это отражалось в метриках (размер очереди или время сервера на обработку). Это облегчает последующую интерпретацию результатов эксперимента.


Этапы реализации (пошаговый план)
Чтобы разработать и подготовить сервис B к тестированию, нужно выполнить следующие шаги:
Инициализация проекта: Создать новый Spring Boot-приложение для сервиса B. Подключить необходимые зависимости:


Spring Web (для создания REST API и обработки HTTP-запросов).


Spring Boot Actuator (для экспонирования метрик и состояния здоровья).


Micrometer Prometheus Registry (для интеграции метрик с Prometheus).


Убедиться, что версия Java и используемых библиотек совместима с остальными сервисами (A и C).


Разработка REST API: Реализовать контроллер(ы) в сервисе B. Например, создать контроллер TargetController с методом, обрабатывающим запрос GET /api/test:


Метод не принимает никаких параметров (или минимально необходимую информацию из запроса).


Внутри метода можно добавить небольшую фиктивную задержку или логирование для имитации обработки.


Метод возвращает ResponseEntity<String> со статусом 200 и телом, содержащим, например, строку "OK" (или другой фиксированный ответ).


POST-методы для API сервиса B в базовой версии не используются.


Обработка исключений: Добавить глобальный обработчик ошибок (@ControllerAdvice) или использовать механизмы Spring Boot для обработки неожиданных ситуаций. Цель: любые неперехваченные исключения в логике сервиса B должны приводить к возвращению клиенту HTTP-кода 5xx, чтобы Prometheus мог зафиксировать ошибочный ответ. Также зафиксировать эти события в логах.


Настройка метрик: Включить все необходимые метрики через Actuator:


В файле настроек приложения (например, application.yml) указать management.endpoints.web.exposure.include: health,prometheus, чтобы эндпоинты метрик и здоровья были доступны по HTTP.


Опционально настроить собрание дополнительных метрик (если требуются специальные показатели). Например, используя @Timed аннотации для конкретных методов или вручную инкрементируя счётчики Micrometer в коде, если нужно отследить какое-то событие. В рамках базового прототипа достаточно встроенных метрик HTTP.


Запустить приложение локально и проверить: переход по http://localhost:8081/actuator/health должен вернуть статус "UP", а по http://localhost:8081/actuator/prometheus – список метрик. Убедиться, что там присутствуют метрики HTTP-запросов (они появятся после хотя бы одного обращения к API B).


Docker-упаковка: Написать Dockerfile для сервиса B. Например:


Использовать официальный образ JDK в качестве базового.


Скопировать собранный .jar файл сервиса B в образ.


Установить точку входа: ENTRYPOINT ["java","-jar","/app/service-b.jar"].


Expose 8081 порт.


Собрать образ командой docker build -t service-b:latest . и протестировать локально: запустить контейнер, проверить через docker logs и curl, что сервис стартовал и отвечает на эндпоинты.


Интеграция в docker-compose: Добавить сервис B в docker-compose файл вместе с остальными:


Объявить сервис B, использующий собранный образ, открыть порт 8081 (можно без публикации наружу, только в сети).


Убедиться, что сервис C (лимитирующий прокси) ссылается на сервис B: например, через переменные окружения или конфигурационный файл для C указать URL сервиса B как backend (например, http://service-b:8081).


Перезапустить всю тестовую платформу (A, C, B, Redis, Prometheus) и убедиться, что все сервисы поднялись и видят друг друга. Сервис C в режиме без ограничения должен проксировать все запросы на B, что проверяется простым вызовом к сервису A (генератору): если A отправляет 1 тестовый запрос, C пропускает его, B получает и отвечает (см. логи B или метрики счётчика запросов увеличились).


Тестирование сценариев нагрузки: После успешной интеграции провести серию нагрузочных испытаний согласно сценариям:


Сценарий 1: Равномерная нормальная нагрузка. Настроить сервис A генерировать стабильный поток запросов с частотой, не превышающей лимит, установленный в C. Например, если лимит 100 запросов/с, генерировать 80–100 запросов/с равномерно. Ожидаемый результат:


Сервис C практически не отклоняет запросы (процент 429 близок к 0).


Сервис B получает весь поток, успешно отвечает на все запросы. Среднее время отклика B остаётся низким и почти постоянным, ошибки 5xx отсутствуют.


Эти метрики фиксируются Prometheus (график времени отклика ровный, throughput равен генерации, B стабилен).


Сценарий 2: Кратковременные всплески трафика. Настроить генератор A так, чтобы чередовались периоды нормальной нагрузки и короткие пики, превышающие лимит в несколько раз. Например: 30 секунд при 100% лимита, затем 10 секунд при 300% от лимита, снова спад. Провести тест для разных алгоритмов лимитирования в C (фиксированное окно, скользящее окно, токен-бакет). Наблюдения/требования:


Сервис C должен сглаживать нагрузку: при фиксированном окне возможен краткий всплеск для B на границе окна; при скользящем окне нагрузка на B выравнивается; токен-бакет позволит B принять чуть больше запросов, пока есть накопленные токены, но тоже ограничит длительность пика.


Сервис B под такими всплесками не падает: время отклика может увеличиваться во время пиков, но после них возвращается к норме. На графиках метрик должны прослеживаться всплески latency, соответствующие допущенному трафику. Процент ошибок на B при грамотном лимитировании остаётся минимальным.


Проверить работу интеллектуального модуля: если он включён, возможно, перед ожидаемым пиком он увеличит лимит (тогда B примет больше запросов, пик сгладится менее строго) или снизит лимит (тогда B будет ещё лучше защищён, приняв совсем мало запросов во время всплеска). Отразить в выводах, как это сказалось на метриках B.


Сценарий 3: Аномальная нагрузка, превышение лимитов (имитация DDoS). Настроить сервис A или нескольких агрессивных клиентов отправлять запросы с частотой намного выше лимита. Цель – проверить, что большинство избыточных запросов отклоняется сервисом C, и сервис B не теряет устойчивость. Ожидания:


Процент отклонённых запросов (HTTP 429) в C будет очень высоким (близким к той доле, на которую нагрузка превышает лимит). Например, при 500% нагрузке и лимите 100%, ~80% запросов должны отсеяться.


Сервис B в это время обрабатывает только разрешённые ~20% запросов. По метрикам B: throughput соответствует лимиту, время отклика может немного возрасти из-за близости к максимальной нагрузке, но существенных ошибок нет. B сохраняет работоспособность, тогда как без лимитирования он бы обязательно перегрузился.


Дополнительно проверяется, не приводит ли интенсивная работа лимитера к деградации самого C (это видно косвенно – если C начинает тормозить, то и поток до B уменьшится не только из-за отброса лишнего, но и из-за просадок). Однако для ТЗ на B достаточно отметить, что B защищён от DDoS-подобной нагрузки за счёт фильтрации C.


Сценарий 4: Сбой хранилища или узла (тест отказоустойчивости). Во время одного из тестов намеренно отключить Redis (эмуляция сбоя общего хранилища лимитеров) или притормозить работу сервиса C. При отключении Redis сервис C должен перейти в режим fail-open и перестать лимитировать запросы. Это экстримальный случай, когда весь входящий поток обрушивается на сервис B:


Требование: сервис B должен, насколько возможно, оставаться доступным. В его логах/метриках ожидается значительный рост времени обработки, возможно возникновение ошибок 5xx (например, если очередь запросов переполнится). Однако система не должна полностью «лечь»: хотя бы часть запросов B продолжает обслуживать.


После восстановления связи с Redis (или перезапуска Redis) сервис C возобновит нормальное ограничение трафика. Сервис B должен быстро восстановиться – время отклика вернётся к норме, ошибки прекратятся. В метриках это будет видно как всплеск показателей во время сбоя, затем возврат к стабильной линии.


Такой сценарий демонстрирует баланс между доступностью и защищённостью: политика fail-open выбрана, чтобы не блокировать все запросы при сбое лимитера. ТЗ для B в этой ситуации – пережить кратковременную перегрузку и вернуться к нормальному режиму, что и будет проверено экспериментально.


Анализ результатов: После выполнения всех тестов собрать ключевые метрики для сервиса B по каждому сценарию и алгоритму лимитирования:


Среднее время отклика B в норме и на пике, скорость восстановления.


Доля отклонённых запросов (429) в системе и какова нагрузка на B при этом.


Наличие/отсутствие отказов сервиса B (падений, перезапусков) и процент ошибок 5xx.


Эти данные будут использованы для вывода заключений о «уровне защиты от перегрузок». Если сервис B во всех случаях оставался стабильным и быстродействующим (за исключением контролируемых увеличений времени отклика), значит, лимитирование работало эффективно. Если же при каком-то алгоритме B страдал (например, фиксированное окно пропускало слишком много burst-трафика и B сильно тормозил), это будет учтено как недостаток данного метода лимитирования.


В итоге реализация сервиса B по данному техническому заданию обеспечит надежную основу для экспериментов с нагрузкой. Сервис B будет предоставлять чёткие измеримые показатели, позволяющие сравнить алгоритмы ограничения трафика и подтвердить необходимость адаптивной настройки параметров лимитирования в режиме реального времени.



____________________________________________________________________________

Техническое задание на разработку сервиса C (прокси лимитирования HTTP-запросов)
1. Общий обзор и назначение

Сервис C представляет собой отдельный прокси-сервис, выполняющий ограничение частоты HTTP-запросов (rate limiting) перед передачей их к сервису B. Он расположен между сервисом A (генератор нагрузки) и сервисом B, принимая все HTTP-запросы от A и решая, пропустить их дальше или отклонить. Основная цель – защитить сервис B от перегрузки и проанализировать поведение различных алгоритмов лимитирования под нагрузкой в рамках дипломного проекта.

Сервис C не встроен в B, а функционирует как самостоятельный компонент (аналогично API Gateway). При поступлении запроса прокси либо перенаправляет его к сервису B (если лимит не превышен), либо возвращает клиенту ответ с кодом 429 Too Many Requests в случае превышения лимита. Таким образом, сервис C играет исследовательскую и аналитическую роль: позволяет сравнивать эффективность алгоритмов ограничения частоты в распределенной системе без изменения кода сервиса B.

Основные возможности сервиса C:

Реализация нескольких стратегий rate limiting – Fixed Window, Sliding Window и Token Bucket – для гибкого управления трафиком.

Централизованное хранение состояния лимитеров в Redis, чтобы сохранять состояние между запросами и перезапусками.

REST API для конфигурации параметров лимитирования на лету (изменение лимитов, переключение алгоритма и пр.).

Интеграция с внешним интеллектуальным модулем (на Python) через REST API для автоматической подстройки лимитов (например, увеличение порога при прогнозируемом росте нагрузки).

Экспорт подробных метрик Prometheus для мониторинга: количество запросов, срабатывания лимита, время обработки, состояние Redis и др.

Stateless-архитектура – отсутствие привязки к локальному состоянию, что упрощает развёртывание и перезапуск сервиса.

Логирование ключевых событий (конфигурация, ошибки, превышение лимита и т.д.) в stdout в структурированном формате для последующего анализа.

2. Поддерживаемые алгоритмы ограничения частоты

Сервис реализует три основных алгоритма ограничения частоты запросов. Алгоритм можно переключать динамически для сравнения в процессе тестирования. Все алгоритмы используют хранилище Redis для учета запросов и хранения счетчиков/токенов, что обеспечивает хранение состояния лимитирования за пределами процесса.

Fixed Window (фиксированное окно): самый простой подход – подсчитывается количество запросов в фиксированных дискретных интервалах времени (например, минутных или секундных окнах). Если число запросов превысило заданный лимит в пределах текущего окна, последующие запросы до конца окна отклоняются (код 429). При наступлении нового окна счетчик обнуляется. Преимущество: простота реализации (один ключ Redis с счетчиком и заданным TTL) и понятность политики лимитирования (например, «не более 100 запросов в минуту» трактуется буквально). Недостаток: эффект «границы окна» – клиент может послать максимальное число запросов в самом конце одного окна и сразу столько же в начале следующего, фактически осуществив короткий всплеск, превышающий лимит почти вдвое.

Sliding Window (скользящее окно): более точный алгоритм, учитывающий трафик за скользящий интервал времени продолжительностью N секунд, отсчитываемый относительно текущего момента, а не от границ фиксированных интервалов. При каждом новом запросе вычисляется, сколько запросов было за предыдущие N секунд; если превышен лимит, запрос отклоняется. Такой подход сглаживает распределение запросов и устраняет резкие скачки нагрузки на границах интервалов. Недостатки: реализация сложнее fixed window и требует хранения большего объема данных в Redis (например, временных меток запросов или ведения нескольких скользящих счетчиков), что увеличивает накладные расходы. Возможные реализации включают использование Sorted Set в Redis для хранения таймстампов запросов или комбинированный счетчик с двумя окнами (текущее и предыдущее) для аппроксимации скользящего подсчета.

Token Bucket (ведро токенов): алгоритм, допускающий контролируемые всплески трафика. Имеется «ведро» токенов фиксированной ёмкости (например, capacity = 100 токенов); токены пополняются в ведро с определенной постоянной скоростью (например, fillRate = 10 токенов в секунду) до максимальной ёмкости. Каждый входящий запрос потребляет 1 токен из ведра. Если в момент поступления запроса токенов не осталось, запрос отклоняется с 429. Преимущества: обеспечивается средняя пропускная способность не выше заданной скорости, при этом разрешаются кратковременные bursts трафика (до исчерпания токенов), что полезно для улучшения UX (короткие серии запросов не сразу блокируются). Недостаток: более сложная реализация – требуется точно рассчитывать время пополнения токенов и хранить состояние ведра (количество оставшихся токенов и время последнего обновления). В Redis для Token Bucket обычно хранится одна запись (например, Hash с полями tokens и last_timestamp), которая обновляется атомарно при каждом запросе.

Во всех режимах сервис C хранит счетчики и другие данные алгоритмов в Redis. Это обеспечивает согласованность лимитирования и устойчивость к сбоям (Redis выступает как центральное отказоустойчивое хранилище). Атомарные операции Redis (например, INCR с EXPIRE, Lua-скрипты) используются для корректного подсчета запросов без гонок между конкурентными запросами. Параметры лимитирования (порог, окно, скорость заполнения и т.д.) могут изменяться без перезапуска сервиса через конфигурационный REST API (см. раздел 4), что позволяет динамически переключать алгоритмы и их настройки в процессе тестирования.

3. Архитектура сервиса и компоненты

Архитектура сервиса C модульная, что облегчает понимание и расширение системы. Основные компоненты прокси-сервиса:

HTTP Proxy Handler – компонент, принимающий входящие HTTP-запросы от сервиса A. Реализует простейший веб-сервер, слушающий заданный порт (8082) и обрабатывающий запросы. Для каждого запроса вызывает модуль RateLimiter для принятия решения. В случае «allow» Proxy Handler пересылает запрос к целевому сервису B (при сохранении метода, URL, заголовков и тела запроса), ожидает ответ от B и возвращает его клиенту A. В случае «deny» Proxy Handler сразу возвращает HTTP 429 с описанием превышения лимита. Примечание: в реализационном плане, пересылка к B может быть сделана с помощью встроенного HTTP-клиента или библиотеки проксирования; для простоты сервис B предполагается доступным по конфигурируемому targetUrl (базовый URL без пути).

Маршрутизация прокси: сервис C обрабатывает собственные эндпоинты конфигурации (/config/...) и Actuator (/actuator/health, /actuator/prometheus) локально. Все остальные HTTP-запросы, полученные от сервиса A, проксируются в сервис B. Проксирование сохраняет метод, путь, query-параметры, заголовки и тело запроса. Итоговый адрес формируется как targetUrl + входящий путь и query.

RateLimiter (модуль ограничения) – ядро сервиса, реализующее логику трех алгоритмов ограничения. Инкапсулирует стратегии FixedWindowStrategy, SlidingWindowStrategy и TokenBucketStrategy (например, через шаблон Strategy или аналогичный механизм). RateLimiter получает от Proxy Handler информацию о текущем запросе и выполняет проверку лимитов по одному глобальному счетчику для всего трафика (без учета идентификатора клиента/IP) через обращение к Redis. В зависимости от активного алгоритма, RateLimiter либо увеличивает счетчик запросов в текущем окне, либо вычисляет количество запросов в скользящем интервале, либо обновляет число токенов в ведре. Решение (пропустить или отклонить) возвращается обратно в Proxy Handler. Все операции с Redis выполняются атомарно, чтобы параллельные запросы не нарушали логику. Например, для Fixed Window используется INCR с TTL, для Token Bucket – скрипт, вычисляющий пополнение и выдачу токена в одном вызове.

RedisClient (клиент хранилища) – вспомогательный компонент, отвечающий за подключение к Redis и выполнение команд. Поддерживает пул соединений или единое долговременное соединение. Реализует операции чтения/записи в Redis для каждого алгоритма (возможно, через вызов Lua-скриптов для консистентности). Также собирает статистику по времени ответа и ошибкам Redis для метрик. При недоступности Redis генерирует событие об ошибке и информирует RateLimiter о невозможности выполнить проверку (см. раздел 7 о поведении fail-open).

ConfigController (REST API управления) – компонент, отвечающий за обработку входящих REST-запросов, предназначенных для настройки сервиса. Предоставляет API (см. раздел 4) для установки новых параметров лимитирования, переключения алгоритмов и просмотра текущей конфигурации. ConfigController валидирует входящие JSON-конфигурации и применяет их: обновляет текущие настройки RateLimiter (например, меняет активный алгоритм, пределы, окно и т.д.). Рекомендации от интеллектуального модуля проходят через тот же механизм валидации и применения, но поступают как ответ на обращение сервиса C к ИИ-модулю. При необходимости актуальная конфигурация хранится в Redis под ключом (например, ratelimiter:config); сервис читает этот ключ при старте и применяет изменения при обновлении.

MetricsCollector – компонент, собирающий и обновляющий метрики для экспорта Prometheus. При каждом запросе MetricsCollector увеличивает счетчики общего числа запросов, отклоненных/пропущенных, замеряет время обработки (с помощью таймеров вокруг вызова RateLimiter и запроса к B), записывает задержки Redis. Метрики экспонируются через Spring Boot Actuator на эндпоинте /actuator/prometheus. MetricsCollector также может получать текущие значения конфигурации от RateLimiter (например, текущий лимит, размер окна и пр.), чтобы выставлять соответствующие gauge-метрики.

Logger – компонент логирования, осуществляющий запись структурированных логов на консоль (stdout). Используется во всех частях сервиса для фиксации важных событий: запуск сервиса, изменение конфигурации, переключение алгоритма, отказ в запросе по лимиту, ошибки соединения с Redis, восстановление соединения и т.д. Логгер поддерживает уровни важности (INFO, WARN, ERROR) и единый формат сообщений (см. раздел 8).

Дополнительно, структура сервиса предполагает, что он stateless: между запросами не сохраняется никакого состояния в памяти, кроме кэша конфигурации. Вся необходимая для принятия решения информация хранится в Redis (или приходит с каждым запросом). Это упрощает перезапуск и развёртывание сервиса без потери состояния лимитирования.

4. REST API интерфейс

Сервис C предоставляет ряд REST-эндпоинтов для управления лимитированием. Эти интерфейсы предназначены для операторов теста или внешних модулей автоконфигурации. В целях упрощения безопасности авторизация и аутентификация запросов не выполняются (предполагается использование во внутренней тестовой среде). Формат обмена данными – JSON для запросов и ответов, если не указано иное.

Основные эндпоинты REST API:

POST /config/limits – установка параметров лимитирования. Принимает JSON с конфигурацией (см. раздел 5 для структуры данных). Позволяет задавать предельную частоту запросов, размер окна (для алгоритмов fixed/sliding) или параметры токен-бакета. Если в теле указано другое значение алгоритма, происходит переключение текущего алгоритма на новый. В ответ сервис возвращает подтверждение (HTTP 200 OK и текущую активную конфигурацию либо 400 Bad Request при ошибке в данных).

GET /config/limits – получение текущей конфигурации лимитирования. Возвращает JSON с активным алгоритмом и всеми соответствующими параметрами (текущий лимит, окно или емкость ведра и скорость заполнения). Это позволяет в любой момент узнать, какие ограничения сейчас применяются.

POST /config/algorithm – переключение алгоритма лимитирования на лету. Принимает JSON с полем algorithm (значения: "fixed", "sliding", "token"), либо в качестве параметра URL. При вызове данного метода сервис переключает активный алгоритм на указанный. Параметры лимита при этом могут использовать сохраненные ранее значения или некоторые значения по умолчанию (например, при переходе на Token Bucket можно использовать последние заданные capacity и fillRate, либо требовать их указания вместе с алгоритмом). Примечание: как правило, предпочтительно вызывать POST /config/limits с полным набором параметров, чтобы явно задать и алгоритм, и его настройки одновременно. Данный эндпоинт выделен для удобства, если требуется быстро изменить только алгоритм, не меняя числовые лимиты (например, в ходе эксперимента).

POST /reload (опционально) – мягкая перезагрузка конфигурации. Может использоваться для применения конфигурационных изменений из внешнего источника (например, если мы храним конфиг в Redis или файле). В рамках данного проекта может не понадобиться, так как есть прямые методы установки параметров через /config/limits.

Все REST-запросы, изменяющие конфигурацию, применяют новые параметры немедленно и влияют на дальнейшую обработку запросов. Для безопасности в контексте дипломного проекта аутентификация не реализована, но предполагается, что эти методы доступны только доверенным компонентам или операторам теста. Для наблюдаемости используются стандартные Actuator-эндпоинты /actuator/health и /actuator/prometheus.

5. Формат JSON-конфигурации лимитов

Для задания параметров лимитирования используется формат JSON. Он универсален для всех алгоритмов – некоторые поля применяются только к определенным типам алгоритмов. Ниже описаны ключевые поля конфигурации и их смысл:

algorithm (string) – выбираемый алгоритм ограничения: "fixed", "sliding" или "token". Это поле обязательно. Если указано значение, отличающееся от текущего активного алгоритма, сервис переключится на новый режим лимитирования.

limit (integer) – числовой лимит запросов. Для fixed и sliding указывает максимальное число запросов за указанный интервал времени. Для token может игнорироваться (вместо него используются capacity и fillRate), либо трактоваться как максимальная средняя скорость (RPS), но в данной реализации основную роль играют поля capacity и fillRate. Пример: "limit": 100 – не более 100 запросов за окно (fixed/sliding).

window (integer) – размер окна в секундах для алгоритмов с окном (fixed, sliding). Определяет длительность интервала подсчета запросов. Для fixed window это размер фиксированного окна, для sliding window – длина скользящего интервала. Для token bucket это поле не используется. Пример: "window": 60 – окно 60 секунд.

capacity (integer) – ёмкость ведра токенов (максимальное число токенов) для алгоритма token bucket. Определяет, сколько запросов подряд можно обработать вburst-режиме. Для других алгоритмов не применяется. Пример: "capacity": 100 – ведро максимум на 100 токенов (т.е. не более 100 запросов сразу при пустом ведре).

fillRate (integer) – скорость пополнения токенов (токенов в секунду) для алгоритма token bucket. Определяет среднюю пропускную способность. Например, 5 токенов в секунду соответствует в среднем 5 запросам в секунду на длинном интервале. Для fixed/sliding не используется. Пример: "fillRate": 5 – добавляется 5 токенов в секунду в ведро (с capacity, например, 100).

burst (integer, опционально) – альтернативное название для capacity, может использоваться для ясности, обозначая максимально допустимый всплеск запросов сверх средней нормы. Если указано, может дублировать значение capacity.

Поле algorithm и связанные с ним параметры должны согласовываться. Если, к примеру, algorithm = "fixed", сервис ожидает наличие полей limit и window (и игнорирует capacity/fillRate, если они есть). При несоответствии (например, указаны поля token bucket для алгоритма fixed) сервис может вернуть ошибку 400. Допускается, однако, указывать сразу все поля – тогда сервис возьмет только релевантные для выбранного алгоритма.

Пример JSON-конфигурации: конфигурация для токен-бакета с емкостью 50 и скоростью 5 RPS:

{
  "algorithm": "token",
  "capacity": 50,
  "fillRate": 5
}


Другой пример, для фиксированного окна 60 секунд с лимитом 100 запросов:

{
  "algorithm": "fixed",
  "limit": 100,
  "window": 60
}


В ответ на GET /config/limits сервис вернет текущую конфигурацию в подобном JSON-формате. В случае интеграции с внешним интеллектуальным модулем, такие JSON возвращаются им в ответе на POST /v1/limit-config, а сервис C применяет их по тем же правилам. Например, модуль может рекомендовать увеличение поля limit или capacity, если прогнозирует рост входящего RPS, либо переключить algorithm при обнаружении, что другой метод справится лучше.

6. Метрики Prometheus

Сервис C экспонирует подробные метрики для системы мониторинга Prometheus через Spring Boot Actuator (/actuator/prometheus). Для этого подключаются spring-boot-starter-actuator и micrometer-registry-prometheus, а в конфигурации включаются endpoints health,prometheus. Ниже перечислены основные метрики и их семантика (названия на английском, как принято в Prometheus):

ratelimiter_requests_total{decision="[forwarded|rejected]"} – счетчик (Counter) общего числа обработанных сервисом C запросов, с разбивкой по результату: "forwarded" – запросы, пропущенные к сервису B, "rejected" – запросы, отклоненные (ответ 429). Например, ratelimiter_requests_total{decision="forwarded"} покажет количество запросов, прошедших через лимитер.

ratelimiter_request_duration_seconds – гистограмма (Histogram) времени обработки запросов сервисом C в секундах. Включает в себя время выполнения всех операций для одного запроса: задержка в очереди (если есть), время проверки лимита (включая запрос к Redis) и время проксирования запроса до ответа от сервиса B. По этой метрике рассчитываются среднее время ответа и перцентили (p95, p99 и т.д.). Например, p95 время обработки можно получить из гистограммы с помощью функций Prometheus (histogram_quantile(0.95, ...)). Метрика помогает оценить накладные расходы, вносимые прокси-лимитером.

ratelimiter_requests_by_algorithm_total{algorithm="[fixed|sliding|token]"} – счетчики запросов, обработанных под управлением каждого алгоритма. При переключении алгоритмов на лету данные метрики позволяют понять, сколько запросов было обработано в режиме Fixed Window, сколько в Sliding Window, сколько в Token Bucket. Реализовано либо как отдельные счетчики для каждого алгоритма, либо как единая метрика с лейблом algorithm. Например, ratelimiter_requests_by_algorithm_total{algorithm="fixed"} увеличивается при работе в режиме фиксированного окна. Суммарно по всем алгоритмам значение этой метрики будет равно ratelimiter_requests_total.

ratelimiter_current_limit – текущий лимит запросов в единицу времени (Gauge). Отражает активное значение порога для лимитирования. Для алгоритма fixed/sliding это количество запросов за окно, приведенное к, например, секунде или прямо за окно (можно экспонировать как «100 запросов за 60 секунд», но Prometheus метрики — численные, поэтому обычно либо нормализуют до запросов в секунду, либо экспонируют оба значения раздельно). В простом варианте ratelimiter_current_limit может показывать текущее значение limit (например, 100). Для Token Bucket, где лимит задается парой значений, можно или не использовать эту метрику, или отображать эквивалентную среднюю скорость (например, fillRate).

ratelimiter_window_seconds – размер окна в секундах для текущего алгоритма (Gauge). Имеет значение для fixed/sliding алгоритмов; для Token Bucket может быть 0 или не определена. Помогает понимать контекст current_limit. Например, если ratelimiter_current_limit=100 и ratelimiter_window_seconds=60, это соответствует 100 запросам в минуту.

ratelimiter_bucket_capacity – емкость токен-бакета (Gauge). Отображается, если активен алгоритм Token Bucket (например, 50 токенов). Если сейчас выбран другой алгоритм, метрика может либо не экспонироваться, либо принимать значение последней использованной емкости/0. Эта метрика, наряду со следующей, актуальна для понимания настроек Token Bucket.

ratelimiter_token_fill_rate – скорость пополнения токенов в секунду (Gauge) для алгоритма Token Bucket. Например, 5 (токенов/сек). Аналогично, вне режима Token Bucket значение не обновляется или равно 0.

(Примечание: В реализации может оказаться удобным экспортировать текущие лимитирующие параметры одним информативным способом — например, метрикой с лейблами для разных параметров. Однако для наглядности в рамках ТЗ они перечислены раздельно.)

ratelimiter_redis_request_duration_seconds – гистограмма времени выполнения операций с Redis. Каждое обращение к Redis (например, вызов скрипта или команды INCR) замеряется, и распределение латентности собирается в этой метрике. Позволяет выявить задержки на стороне хранилища состояния.

ratelimiter_redis_errors_total – счетчик ошибок при работе с Redis. Увеличивается при любых неудачных попытках чтения/записи в Redis (таймауты, разрывы соединения, ошибки команд). Помогает мониторить стабильность хранилища.

ratelimiter_redis_connected – текущий статус подключения к Redis (Gauge типа 0/1). Значение 1 означает, что соединение с Redis установлено и работоспособно, 0 – соединение потеряно. В нормальной ситуации метрика равна 1; при сбое на стороне Redis она перейдет в 0, сигнализируя системе мониторинга о проблеме. Восстановление соединения снова установит 1.

Помимо перечисленных, сервис может экспонировать и другие метрики для полноты картины: например, счетчик HTTP-ответов по кодам (200/429), метрики памяти/GC (если используется JVM), и др. Однако в рамках данного ТЗ ключевыми являются метрики, касающиеся производительности и правильности работы механизма лимитирования.

7. Поведение при недоступности Redis (fail-open)

Redis выступает критически важным компонентом для работы сервиса C. Тем не менее, предусмотрена стратегия работы при отказе или недоступности Redis – режим fail-open. Это означает, что в случае, если сервис C не может выполнить проверку лимита из-за недоступности хранилища (например, потеряно соединение с Redis, истек таймаут обращения или Redis недоступен), прокси не будет блокировать запросы, а напротив, начнет пропускать все входящие запросы напрямую к сервису B. Такой подход предпочтительнее, чем полная остановка выдачи трафика (fail-closed), чтобы сохранить доступность системы для легитимных пользователей даже при сбое механизма лимитирования. Проще говоря, если Redis недоступен, сервис C временно перестает лимитировать и действует как простой прокси без ограничений.

Конкретное поведение в режиме fail-open:

При первом обнаружении проблемы с Redis (ошибка команды или ping) сервис переводит внутренний флаг состояния Redis в «ошибочный». Это может быть реализовано в RedisClient: например, при получении исключения он помечает redisConnected=false.

Все новые входящие запросы, пока флаг в ошибочном состоянии, автоматически считаются разрешенными. RateLimiter сразу возвращает решение "allow" без обращения к хранилищу.

Клиентам (сервису A) в этом режиме сервис C продолжает отвечать с тем же успехом, как если бы лимита не было. Код ответа 429 не выдается независимо от интенсивности запросов.

Логирование: переход в fail-open фиксируется в логах уровня ERROR/WARN с сообщением о недоступности Redis. Также при восстановлении соединения событие фиксируется (INFO).

Метрики: метрика ratelimiter_redis_connected сменится на 0. Счетчик ratelimiter_redis_errors_total будет увеличиваться на каждую неудачную попытку соединения/запроса к Redis. Также можно ввести отдельный gauge-индикатор режима работы (например, ratelimiter_mode{type="failopen"} = 1 при активном fail-open), либо отслеживать опосредованно по комбинации других метрик.

Сервис продолжает периодически пытаться восстановить соединение с Redis в фоне. Механизм восстановления может быть реализован с экспоненциальной задержкой или постоянными попытками ping. Как только Redis становится доступен, сервис возвращается к нормальному режиму (fail-open завершается, вновь применяется лимитирование запросов согласно текущей конфигурации). При этом счетчики/токены в Redis, возможно, «обнулятся» или потеряют актуальность за время простоя – данный эффект допустим, так как основной приоритет – вернуть систему под контроль как можно скорее.

В момент переключения обратно на работу с Redis можно залогировать предупреждение о возможном несогласованном состоянии (например, что накопилось неопределенное количество запросов за период недоступности Redis).

Режим fail-open гарантирует, что сбой вспомогательного компонента (Redis) не нарушит доступность основного сервиса B для клиентов. Такой подход оправдан в контексте нагрузочного тестирования и исследования (лучше получить избыточный трафик, чем потерять его из-за ложных срабатываний лимитера). В боевой среде выбор между fail-open и fail-closed зависит от требований (безопасность vs доступность), но здесь явно указан fail-open.

8. Логирование

Сервис C производит подробное логирование основных событий, что необходимо для отладки и анализа работы прокси-лимитера. Логи пишутся в стандартный вывод (stdout) в однострочном формате, удобном для парсинга. Используются уровни логирования INFO, WARN и ERROR (а также DEBUG при необходимости локальной отладки).

Формат логов:
[timestamp] [level] [component] - message
где timestamp – время события (например, в ISO-8601 или Unix time), level – уровень (INFO/WARN/ERROR), component – источник или модуль сервиса (например, RateLimiter, RedisClient, ProxyHandler), message – текстовое сообщение.

Примеры логов:

[2026-01-19T16:30:00Z] INFO ConfigController - Applied new config: algorithm=sliding, limit=500, window=60 – применена новая конфигурация лимитов через API.

[2026-01-19T16:30:05Z] WARN RedisClient - Redis connection lost, entering fail-open mode – потеряно соединение с Redis, сервис переключается в режим без ограничения.

[2026-01-19T16:30:06Z] ERROR RateLimiter - Request rejected: limit exceeded (global limit) – (при включенном лимитировании) запрос отклонен по причине превышения лимита.

[2026-01-19T16:30:10Z] INFO RedisClient - Redis reconnected, resuming normal operation – восстановлено соединение с Redis, работа лимитера возобновлена.

Содержимое логов и политика:

Уровень INFO: общие события нормальной работы – запуск сервиса (с указанием версии и начальных настроек), применение новой настройки/алгоритма, успешное восстановление соединений, статистика по завершении теста (если релевантно).

Уровень WARN: события, которые не прерывают работу, но могут влиять на эффективность – например, временные таймауты Redis, переход в fail-open, обнаружение нестандартных ситуаций (как слишком долгий ответ от B, если требуется).

Уровень ERROR: серьезные ошибки – невозможность обработать запрос (кроме превышения лимита, которое не считается ошибкой работы сервиса), сбой в компонентах (исключения из Redis клиента, неожиданные ошибки проксирования), иные фатальные проблемы. При ошибках такого рода сервис по возможности продолжает работу (например, при ошибке проксирования конкретного запроса – зафиксирует и продолжит обрабатывать остальные).

Отладочные логи (DEBUG) могут включать подробности исполнения алгоритмов (например, вычисленное число токенов, текущее значение счетчика) и полный контент HTTP-запросов. По умолчанию DEBUG-уровень отключен, чтобы не перегружать вывод при больших объемах трафика.

Каждая запись лога структурирована, чтобы ее можно было разобрать инструментами анализа логов. Например, можно использовать формат JSON для логов (в рамках упрощения проекта допускается и просто единый текстовой формат, как показано выше). Логирование осуществляется в stdout, что соответствует облачной native-практике (контейнеры выводят логи в консоль, откуда они собираются системой оркестрации).

Объем логирования в продакшн-режиме следует ограничивать, однако для исследовательских целей проекта можно логировать более подробно, если это нужно для последующего анализа. В частности, фиксирование каждого отклоненного запроса на уровне INFO может облегчить подсчет вручную, но при высокой нагрузке сильно увеличит объем логов – поэтому вместо этого опираемся на метрики, а в логи выводим агрегированную информацию (например, предупреждение каждые N отклоненных запросов или по истечении окна). Данные решения могут быть настроены по мере необходимости.

9. Развёртывание в Docker и Stateless-архитектура

Сервис C спроектирован как легковесное stateless-приложение, что упрощает его развертывание и перезапуск. Приложение упаковано в Docker-контейнер и не хранит состояния на локальной файловой системе – все динамические данные либо хранятся в Redis, либо в памяти на время выполнения (например, текущая конфигурация). В рамках прототипа используется один экземпляр сервиса.

Docker-развёртывание:

Для сборки используется Dockerfile на базе openjdk (Spring Boot, Java).

При сборке в образ включаются все необходимые зависимости. Конфигурация сервиса (например, адрес Redis) передается через переменные окружения или флаги запуска.

Контейнер запускает сервис на стандартном порту (8082). Порт может быть сконфигурирован через переменную окружения, например PORT или специальный параметр приложения.

Для подключения к Redis используются переменные окружения: например, REDIS_HOST, REDIS_PORT, REDIS_PASSWORD (если требуется). Эти переменные можно задать при запуске контейнера.

Контейнер не требует сохранения состояния, поэтому использует режим запуска stateless (без volume). При перезапуске контейнера он загружает конфигурацию заново (из переменных или по умолчанию) и подключается к Redis, где хранится текущее состояние лимитеров.

Образ запускается как одиночный контейнер; параметры задаются через переменные окружения.

Stateless-архитектура: отсутствие состояния внутри сервиса C означает, что:

Восстановление: В случае падения или перезапуска контейнера сервис возобновляет работу без какой-либо инициализации состояния – достаточно подключения к Redis. Это упрощает отказоустойчивость: можно перезапустить контейнер без последствий для лимитирования (счётчики в Redis продолжают отсчитывать как ни в чем не бывало).

Консистентность конфигурации: Актуальная конфигурация лимитирования может храниться в Redis под ключом (например, ratelimiter:config) и считается единственным источником правды. Сервис читает этот ключ при старте и применяет изменения при обновлении.

10. Ограничения производительности

Сервис C должен быть высокопроизводительным, чтобы не стать узким местом в цепочке A→C→B. Ниже перечислены ключевые требования и ограничения, связанные с производительностью:

Пропускная способность: Прокси-сервис должен выдерживать интенсивный поток запросов. Ориентировочно, один инстанс сервиса C должен обрабатывать порядок нескольких тысяч запросов в секунду без деградации времени ответа. Производительность проверяется нагрузочным тестированием (сервисом A) и должна оставаться стабильной в целевом диапазоне RPS.

Задержка обработки: Добавление прокси-слоя не должно значительно увеличить время отклика для пользователя. Цель – минимизировать накладные задержки: собственная обработка в C (принятие решения по лимиту) должна занимать доли миллисекунды, плюс добавляется сеть до Redis и до сервиса B. В среднем дополнительная задержка на проксирование и проверку лимита должна быть не более нескольких миллисекунд. Метрика ratelimiter_request_duration_seconds (см. раздел 6) позволяет контролировать, что среднее время обработки (mean) остается низким, а 95-й перцентиль (p95) – в приемлемых пределах (например, <50 мс, в зависимости от удаленности Redis и B).

Оптимизация работы с Redis: Так как на каждый запрос, в норме, совершается хотя бы одно обращение к Redis, важно, чтобы эти операции были максимально быстрыми и эффективными. Redis обычно способен выдерживать очень высокое число запросов в секунду на одиночный инстанс. Тем не менее, следует:

Использовать пиринговое размещение: желательно, чтобы Redis находился в той же сети/датацентре, что и сервис C, с минимальной сетевой задержкой.

Применять оптимальные команды. Например, для fixed window – комбинация INCR + EXPIRE (или Lua-скрипт, как предложено выше, чтобы выполнить их атомарно); для sliding window – возможно, два счетчика и команда INCR/EXPIRE, вместо хранения каждого события отдельно, чтобы снизить нагрузку на память; для token bucket – Lua-скрипт, выполняющий расчет пополнения и выдачи токена за один вызов. Такие оптимизации снижают число round-trip’ов к Redis.

Включить pipeline или batching при необходимости, если приходится делать несколько команд подряд – это сократит накладные расходы коммуникации.

Следить за размером данных: хранить только необходимую информацию с TTL, чтобы Redis автоматически очищал старые записи (например, ключи fixed window имеют TTL чуть больше размера окна, ключ ведра токенов TTL больше максимального простоя клиента). Это предотвращает рост потребления памяти.

Производительность Redis: Single-node Redis – потенциальная точка ограничения, т.к. он однопоточен. Однако на практике один узел Redis обычно способен обслужить гораздо больше запросов, чем один узел HTTP-сервиса, благодаря оптимизации на уровне C и отсутствию затрат на сложную логику. В рамках дипломного проекта достаточно одного отказоустойчивого Redis (например, в режиме Primary/Replica с автофейловером), который выдержит необходимую нагрузку.

Ограничения алгоритмов: разные алгоритмы имеют различную стоимость по CPU и памяти:

Fixed Window – самый дешевый: одна операция инкремента в Redis на запрос (плюс редкий установ TTL). Занимает минимум CPU и памяти (1 ключ на каждый интервал).

Sliding Window – более тяжелый: вариант со sliding log (хранение таймстампов запросов) может генерировать большое количество записей в Redis при высоком RPS, что неэффективно. Поэтому предпочтительно реализовать sliding counter – тогда на запрос также будет 1-2 операции (инкремент счетчиков двух окон). Этот алгоритм немного увеличивает нагрузку на CPU (из-за дополнительной логики расчета) и память (хранение двух счетчиков вместо одного), но дает более плавное ограничение. В рамках тестирования следует учитывать, что sliding window может потреблять больше ресурсов, и результаты по метрикам (время обработки) могут отличаться от fixed window.

Token Bucket – промежуточный по нагрузке: на каждый запрос выполняется скрипт или несколько операций (расчет нового кол-ва токенов, возможное обновление timestamp, условная логика). Это дороже, чем простой INCR, но все еще укладывается в миллисекунды на операцию. Памяти требуется один ключ (например, Hash) на каждый ограничиваемый субъект. В тесте, если все запросы относятся к одной «глобальной» корзине, то используется всего 1 ключ Hash.

Эффективность проксирования: помимо лимитирования, сервис C должен эффективно пересылать сами HTTP-запросы. Здесь важно выбрать подходящую технологию: например, асинхронный неблокирующий сервер (чтобы потоки не простаивали во время ожидания ответов от B). Если используется Java, целесообразно применять WebFlux/Netty или аналогичные решения, либо ограничивать размер пула потоков по числу ядер. Также, HTTP-клиент для форвардинга запросов к B должен поддерживать keep-alive соединения, чтобы не тратить время на установление TCP-сессии для каждого запроса. В идеале, время обработки в C в основном тратится на сетевые операции (к B и Redis), а CPU используется минимально.

Тестирование под нагрузкой: в рамках проекта планируется нагрузочное тестирование сервисом A. Необходимо учесть, что сам сервис A и сервис B могут стать факторами, ограничивающими видимую производительность. Поэтому в ходе экспериментов целесообразно постепенно повышать RPS и отслеживать метрики сервиса C:

Убедиться, что при нагрузке ниже порога лимитирования прокси C не добавляет ощутимой задержки (например, медианное время обработки близко к медианному времени ответа B + постоянная ~несколько миллисекунд).

Проверить, что при нагрузке значительно превышающей лимиты сервис C стабильно отклоняет лишние запросы (метрика ratelimiter_requests_total{decision="rejected"} растет соответствующе) и при этом сам не исчерпывает ресурсы (время обработки отклоненных запросов должно оставаться небольшим, а использование CPU/памяти – в пределах допустимого).

В режиме длительной работы сервис C не должен иметь утечек памяти или деградации производительности. Метрики и логи помогут выявить, если, например, Redis начинает отвечать медленнее под нагрузкой или если количество ошибок растет.

Размеры и ресурсы: контейнер с сервисом C должен быть легковесным. Желательно держать потребление памяти сервисом (не считая Redis) на уровне сотен мегабайт или меньше, CPU – пропорционально нагрузке (несколько vCPU при высоком RPS). Для JVM стоит настроить адекватный heap и GC, чтобы паузы не влияли на работу под нагрузкой. Эти аспекты относятся к низкоуровневой оптимизации реализации.

Подводя итог, сервис C должен работать эффективно: добавлять минимальную задержку и полагаться на Redis, который при правильной настройке выдерживает нагрузку лучше, чем отдельный сервис (являясь in-memory storage). Потенциальные узкие места (Redis, сеть) должны мониториться метриками. Все выбранные алгоритмы и архитектурные решения нацелены на то, чтобы сервис C смог выполнять функцию лимитирования без значительного влияния на производительность всей системы.


_________________________________________________________________________

Интеллектуальный модуль и адаптивное лимитирование трафика

Обзор: В состав системы входит внешний интеллектуальный модуль на Python (со встроенной моделью прогнозирования Prophet), предназначенный для предсказания входящего трафика и динамической настройки параметров rate limiting. Интеграция этого модуля с сервисом C позволяет последнему адаптировать политику ограничения запросов в реальном времени на основе прогнозной нагрузки. Ниже описаны формат взаимодействия по REST, частота обновлений, преобразование прогноза в параметры лимитирования, список изменяемых параметров, а также влияние данной адаптации на отказоустойчивость (включая fail-open) сервиса C. Дополнительно приведены примеры JSON-конфигураций, передаваемых ИИ-модулем, и описаны метрики/логи для валидации эффективности адаптивного подхода.

Формат REST-взаимодействия для адаптивной конфигурации

Для обмена данными между интеллектуальным модулем и сервисом C используется REST API с передачей данных и конфигурации в формате JSON. Сервис C периодически обращается к ИИ-модулю и получает рекомендацию в ответе.

Запрос и ответ (C -> ИИ -> C): сервис C с заданным интервалом выполняет HTTP-запрос (например, POST /v1/limit-config) к REST-эндпоинту интеллектуального модуля. В теле передается JSON-снимок состояния, например: timestamp, observedRps, rejectedRate, latencyP95, errors5xx, а также текущие параметры лимитирования (algorithm, limit/window/capacity/fillRate). Состав полей можно расширять, но важно сохранять единый формат. Ответ ИИ-модуля содержит JSON-конфигурацию лимитов. Формат конфигурации каноничен и совпадает с разделом 5: algorithm = "fixed" | "sliding" | "token", и соответствующие параметры limit/window или capacity/fillRate. Дополнительно могут присутствовать predictedRps и validFor — сервис C игнорирует неизвестные поля.

Применение конфигурации: Сервис C, получив JSON с конфигурацией, валидирует его и применяет новые настройки немедленно, без перезагрузки. Параметры алгоритма обновляются в памяти; при необходимости сервис сбрасывает счётчики/токены предыдущего алгоритма и начинает лимитирование согласно новым значениям. Такой подход позволяет менять лимиты «на лету». Например, если ранее действовал алгоритм Fixed Window с окном 60 с и лимитом 1000 req/мин, а новая конфигурация задаёт Token Bucket с ёмкостью 200 и скоростью 50 токенов/с, сервис C переключится на Token Bucket, установив начальное наполнение бакета и далее работая по новой логике.

Периодичность обновлений и обмен данными

Сервис C вызывает /v1/limit-config по расписанию (например, каждые 30–60 секунд, параметр ADAPTIVE_UPDATE_INTERVAL). Если ИИ-модуль недоступен, ответ не получен в пределах таймаута или конфигурация невалидна, сервис C продолжает работать с последней валидной конфигурацией.

Преобразование прогноза нагрузки в параметры лимитирования

ИИ-модуль на основе истории метрик (интенсивность запросов, время отклика, процент отказов и т.п.) строит прогноз входящего трафика (например, ожидаемый RPS – requests per second – в ближайшем периоде). Модель Prophet анализирует временной ряд входящих запросов и выявляет тенденции (рост, спад, сезонные колебания). Полученный прогноз затем переводится в конкретные настройки rate limiter’а:

Увеличение лимита при прогнозе роста: Если ожидается кратковременный скачок нагрузки (например, предстоящий пик RPS), интеллектуальный модуль может рекомендовать временно повысить лимит запросов. Цель – смягчить влияние всплеска: пропустить больше запросов, если система B способна их выдержать, тем самым снизив задержки для клиентов. Например, при установленном лимите 100 запросов/с и прогнозе роста трафика до 150 req/s модуль может предложить поднять лимит до ~150 (или чуть меньше, с запасом) на время пика. В конфигурации это отразится, например, как увеличение поля limit (для фиксированного/скользящего окна) или fillRate/capacity (для токен-бакета) относительно текущих значений.

Снижение лимита при опасном перенапряжении: Если прогноз указывает на нагрузку, значительно превышающую безопасную для сервиса B (например, грядущий всплеск в несколько раз больше текущего лимита), то политика может быть ужесточена. ИИ-модуль в таком случае рекомендует снизить лимит заранее, чтобы сервис C более агрессивно отсеивал трафик и предотвращал перегрузку B. Практически это выражается в уменьшении limit/ёмкости бакета или замедлении fillRate на период ожидаемого пика.

Выбор алгоритма под характер трафика: В некоторых случаях интеллектуальный модуль может посоветовать переключиться на другой алгоритм лимитирования, более подходящий под прогнозируемый паттерн трафика. Например, при ожидании серии частых коротких всплесков выгоднее использовать Token Bucket (позволяет накапливать токены и обрабатывать короткие бусты, чуть превышая средний поток), тогда как для сглаживания длительных повышений нагрузки — Sliding Window. Сервис C изначально спроектирован с возможностью смены алгоритма и его параметров во время работы, поэтому рекомендация algorithm в получаемом JSON может меняться в зависимости от ситуации.

Привязка ко времени: Рекомендации могут быть рассчитаны на определённый период. Например, Prophet может предсказывать суточные циклы нагрузки. В таких случаях конфигурация от ИИ-модуля может содержать указание окна действия новых параметров. Хотя текущая реализация принимает и применяет настройки немедленно, можно расширить её, чтобы планировать возврат к исходным значениям или дальнейшее обновление после указанного времени (например, через поле validFor: 15 минут). В прототипе же предполагается, что каждая полученная конфигурация актуальна до следующего цикла обмена данными, а дальнейшие корректировки будут произведены при очередной отправке данных.

Важно, что любые изменения лимитов вычисляются с учётом возможностей сервиса B и цель не нарушить SLA: даже при повышении лимита система C не должна пропускать больше запросов, чем B гарантированно обработает без деградации. Грубо говоря, ИИ-модуль может иметь встроенные границы: максимальный лимит не превышает заранее определённого порога (например, известной пропускной способности B), а минимальный лимит не опускается ниже значений, необходимых для обслуживания базового нормального трафика. Это предотвращает как недоиспользование (чересчур низкий лимит при относительно лёгкой нагрузке) так и перегрузку (слишком высокий лимит при риске перегрева B).

Динамически изменяемые параметры конфигурации

Адаптивный механизм позволяет менять следующие параметры сервиса C на лету:

Пороговые значения лимитов: Это основная величина, подлежащая изменению. В зависимости от активного алгоритма, меняется либо допустимое число запросов за единицу времени (limit в окне), либо скорость поступления токенов (fillRate) и/или ёмкость бакета (capacity). Эти значения могут корректироваться интеллектуальным модулем в обе стороны (увеличение/уменьшение) для реактивного или проактивного управления потоком запросов.

Размер временного окна: Для алгоритмов фиксированного или скользящего окна модуль может менять window – длительность интервала подсчёта. Например, укорочение окна делает реакцию лимитера более чувствительной к кратким всплескам, тогда как удлинение сглаживает краткосрочные колебания. На практике изменение размера окна применяется редко и с осторожностью, так как существенно меняет поведение алгоритма.

Тип алгоритма: Система допускает динамическое переключение между Fixed Window, Sliding Window и Token Bucket по рекомендации ИИ-модуля. В обновлении JSON поле algorithm может отличаться от текущего, и сервис C выполнит переход. При смене алгоритма происходит реинициализация внутренних счетчиков: например, при переходе от токен-бакета к оконному счетчику предыдущие токены обнуляются, начинается новый отсчёт запросов в окне. Замена алгоритма во время работы — более радикальный шаг, и в прототипе предполагается использовать его в ограниченных случаях (например, при явном выявлении, что другой метод лучше справится с текущим характером трафика). Основной же фокус адаптации – на изменении числовых параметров в рамках текущего алгоритма.

Прочие параметры: Возможна передача и применение дополнительных настроек, если они поддерживаются сервисом C. Например, степень параллелизма обработчика, политика очереди (если реализована), приоритетные клиенты и т.п. Однако, в рамках данной системы C основными изменяемыми параметрами являются только относящиеся к rate limiting.

Динамическая перенастройка параметров происходит без перезапуска сервиса C. Все изменения конфигурации применяются в режиме реального времени через программный интерфейс. Это означает, что можно часто (каждые несколько десятков секунд) обновлять лимиты, не теряя при этом входящие запросы и не нарушая работу прокси.

Важно подчеркнуть, что изменения конфигурации (особенно смена алгоритма) должны протоколироваться и при необходимости ограничиваться по частоте. Чрезмерно частое переключение алгоритмов или сильные колебания лимитов могут затруднить стабильность системы. Рекомендуется устанавливать защитные ограничения: например, не менять алгоритм чаще, чем раз в несколько минут, и не изменять лимит более чем на определённый процент от предыдущего значения без крайней необходимости.

Отказоустойчивость и поведение fail-open при адаптивном управлении

Базовый механизм отказоустойчивости сервиса C – режим fail-open – сохраняется без изменений: если внешний сторедж Redis недоступен, лимитер отключается и начинает пропускать все запросы, чтобы не блокировать трафик из-за внутренней ошибки. Адаптивное лимитирование не должно нарушать эту логику, однако добавляет новый аспект отказоустойчивости, связанный с работой ИИ-модуля:

Недоступность или сбой ИИ-модуля: В случае, если Python-сервис с Prophet не отвечает на запросы (например, недоступен по сети или падает с ошибкой), сервис C не должен блокировать или замедлять обработку запросов. При истечении таймаута обращения к ИИ-модулю или получении ошибки, сервис C просто фиксирует проблему в логах и продолжает использовать последнюю известную конфигурацию лимитирования. Таким образом, система переходит в режим статического лимитирования до тех пор, пока не восстановится связь с интеллектуальным компонентом. Это можно рассматривать как грациозную деградацию: адаптивность временно теряется, но сама функция ограничения продолжает работать на прежнем уровне, обеспечивая защиту B. В худшем случае (если адаптивные настройки ещё ни разу не получались), сервис C будет работать на предустановленных при запуске значениях лимита (или на значениях по умолчанию), что гарантирует хотя бы базовую защиту.

Защита от некорректных рекомендаций: Модуль Prophet теоретически может выдать аномальные или неверные прогнозы, приводящие к неадекватным параметрам (например, слишком высокому лимиту, фактически отключающему защиту, или близкому к нулю лимиту, блокирующему всех клиентов). Сервис C реализует проверку входных параметров на разумные границы. Любое обновление лимита выше определённого максимума (например, выше физической пропускной способности B или заранее заданного потолка) игнорируется или ограничивается этим потолком. Аналогично, слишком малые значения лимита (которые не позволяют обслуживать минимально необходимый трафик, например, <5 req/s в рабочее время) могут быть отклонены. В логах при этом фиксируется предупреждение. Такая валидация предотвращает негативные эффекты от ошибок ИИ-модуля, повышая устойчивость системы.

Совместимость с fail-open (Redis): В сценарии, когда произошёл сбой Redis и сервис C вошёл в режим fail-open, любые получаемые от ИИ-модуля обновления по сути игнорируются, поскольку лимитер временно не применяется. Сервис C может продолжать обмен данными с ИИ-модулем (и логировать рекомендации), но не будет ограничивать трафик до восстановления Redis. После восстановления хранилища C выйдет из fail-open и применит актуальные на тот момент параметры (либо последние успешные от ИИ, либо штатные). Таким образом, динамическая адаптация не препятствует работе fail-open-механизма, и при восстановлении системы гибкие настройки вновь вступят в силу автоматически.

Fail-safe режим для адаптации: По аналогии с fail-open, можно рассмотреть режим fail-safe для самого адаптивного компонента. Если за N циклов обмена данными не удаётся получить новые настройки, сервис C может оповестить об этом (метрика/событие) и продолжить работу на фиксированных параметрах. В крайнем случае, адаптивный режим можно отключить (через конфигурацию флага, например ADAPTIVE_MODE=false), и сервис C будет функционировать как обычный статический лимитер. Такая возможность пригодится, если интеллектуальный модуль испытывает проблемы – чтобы не влиять на продуктивную систему.

В результате внедрение интеллектуальной адаптации не ухудшает отказоустойчивость: сервис C спроектирован так, чтобы в любой момент времени иметь актуальный рабочий лимит, даже если прогнозы временно недоступны. Основная логика защиты (rate limiting) всегда будет действовать, либо с последними динамическими параметрами, либо с резервными статическими настройками. Поведение fail-open при потере Redis остаётся приоритетным механизмом обеспечения доступности: в экстремальной ситуации все запросы пропускаются, и адаптивный модуль этому не мешает.

Примеры JSON-конфигураций от ИИ-модуля

Ниже приведены примеры содержимого JSON, которые интеллектуальный модуль Prophet может отправить сервису C. Эти примеры иллюстрируют различные сценарии адаптивной перенастройки:

1. Повышение лимита перед прогнозируемым всплеском: Предположим, текущий алгоритм – фиксированное окно 1 с, лимит 100 req/s. Prophet предсказывает рост трафика до ~130 req/s в ближайшую минуту. Модуль решает немного повысить лимит, чтобы пропустить больше запросов во время пика (если система B справится). JSON-конфигурация может выглядеть так:

{
    "algorithm": "fixed",
    "window": 1,
    "limit": 130,
    "predictedRps": 130,
    "validFor": 60
}


Пояснение: Алгоритм остаётся fixed с окном 1 с, но допустимое число запросов в секунду увеличено до 130. Поле predictedRps (130) указывает на ожидаемый уровень RPS, а validFor: 60 (секунд) подразумевает, что через 60 с потребуется обновление. Сервис C, получив это, установит новый лимит 130 req/s на фиксированном окне. В течение минуты пиковой нагрузки доля отклонённых запросов снизится, позволяя B принять больше трафика (пик сглажен менее строго). После указанного периода (или по следующему прогнозу) лимит может быть скорректирован обратно вниз.

2. Снижение лимита при ожидаемом перегрузке: Текущий лимит, допустим, 6000 req/мин (≈100 req/s) (алгоритм Sliding Window, окно 60 с). Ожидается резкий всплеск до 300 req/s, что втрое превышает возможности B. ИИ-модуль предупреждает о риске перегрузки и рекомендует усилить ограничение, временно снижая пропускную способность до ~80 req/s. Конфигурация:

{
    "algorithm": "sliding",
    "window": 60,
    "limit": 4800,
    "predictedRps": 300,
    "validFor": 30
}


Пояснение: Алгоритм sliding сохранён, окно 60 с, но лимит резко уменьшен до 4800 в минуту (≈80 RPS средне, что значительно жёстче прежнего ограничения). Предполагается, что в ближайшие 30 с нагрузка возрастёт до 300 RPS; благодаря снижению лимита сервис C заранее начнёт отбрасывать большую часть запросов (ожидаемо ~70–80% избыточного трафика будет отклонено), пропуская лишь безопасные ~80 req/s. Это защитит сервис B: он будет обрабатывать только допустимый поток, без перегрузки. После прохождения всплеска (через 30 с) модуль может вернуть лимит к исходному уровню ~100 req/s (6000 в минуту) или иному актуальному значению.

3. Смена алгоритма на Token Bucket для бурстового трафика: Предположим, изначально использовался Fixed Window (60 с, 1000 запросов/мин). Выявлено, что трафик носит порывистый характер (частые короткие бусты), и Prophet прогнозирует продолжение такого паттерна. Модуль решает переключить алгоритм на токен-бакет, чтобы более гибко реагировать на bursts. Конфигурация может быть следующей:

{
    "algorithm": "token",
    "capacity": 200,
    "fillRate": 50,
    "predictedRps": 50
}


Пояснение: Алгоритм token с ёмкостью бакета 200 токенов и скоростью пополнения 50 токенов/с. Это эквивалентно среднему лимиту ~50 req/s, но позволяет накапливать до 200 запросов «запаса». То есть, если происходит резкий burst, сервис C сможет сразу пропустить до 200 запросов (пока есть токены), после чего будет пополнять бакет со скоростью 50 токенов/с. Данная настройка сглаживает порывистую нагрузку: краткосрочные всплески до 4 секунд длиной пройдут без жесткого обрыва (пока расходуются накопленные токены), а длительные перегрузки всё равно ограничиваются средней планкой 50 req/s. В приведённом JSON predictedRps = 50 как ожидаемая средняя нагрузка (для ориентира), а поля capacity и fillRate заданы исходя из анализа недавних всплесков. После переключения алгоритма сервис C автоматически очистит старые счётчики окон и начнёт работать по токен-бакету.

Каждый такой JSON-документ сохраняется для аудита (например, в логах сервиса C) и может также экспонироваться через отдельный эндпоинт (только для чтения), чтобы операторы могли видеть текущие активные ограничения.

Метрики и логирование для оценки эффективности адаптивного подхода

Чтобы подтвердить пользу от внедрения интеллектуального модуля и динамического изменения лимитов, необходимо собирать и анализировать соответствующие метрики и логи:

Метрики нагрузки и производительности: Уже существующие метрики, экспонируемые сервисами (сбор через Prometheus), позволяют отслеживать влияние адаптации:

RPS и throughput: Фактическая интенсивность запросов, проходящих через сервис C и достигающих сервиса B. Сравнение графиков RPS B с и без адаптивного лимитирования покажет, удалось ли системе пропустить больше полезного трафика и/или лучше защитить B во время пиков.

Процент отклонённых запросов (HTTP 429): Ключевой показатель работы лимитера. При адаптивном управлении ожидается, что процент 429 будет меняться динамически – например, снижается перед допустимым ростом нагрузки (при повышении лимита) и резко возрастает во время угрозы перегрузки (если лимит снижен). Эти колебания должны коррелировать с прогнозами Prophet. В метриках Prometheus (например, счётчик или gauge для 429) видно, как интеллектуальный модуль сглаживает всплески: в сценариях с модулем форма графика 429 иная, чем без него.

Время отклика и ошибки сервиса B: Главная цель лимитирования – сохранить стабильность B. Поэтому метрики сервиса B (среднее и 95-й процентиль времени ответа, число HTTP 5xx ошибок) сравниваются между режимами. Эффективная адаптация должна приводить к более стабильному времени отклика B при переменном трафике и меньшему числу сбоев. Например, при включённом интеллектуальном модуле пиковое время ответа B может быть ниже, чем без него, либо B вообще избегает падения там, где без адаптации наблюдались бы ошибки из-за перегрузки.

Текущие значения лимитов: Рекомендуется добавить кастомную метрику (gauge) в сервис C, экспонирующую текущий установленный лимит и тип алгоритма. Например, rate_limit_current_limit (значение) и rate_limit_algorithm (лейбл). По графику этого gauge в сочетании с другими метриками можно отслеживать моменты изменения конфигурации и их эффект. Если видно, что в момент повышения limit throughput B увеличился без роста ошибок – значит адаптация позволила улучшить пропускную способность. Если при снижении лимита заметно падение latency на B – значит модуль успешно предвосхитил перегрузку.

Логи сервиса C (адаптивные события): Сервис C должен подробно логировать все изменения, связанные с интеллектуальным модулем:

При каждом запросе /v1/limit-config – запись вида: “AI request sent (interval X s)...”. При получении рекомендации – “AI update applied: algorithm=..., limit/from … to …, window=..., fillRate=..., reason: forecast=... req/s”. Такие логи позволяют проследить хронологию действий: что рекомендовал модуль, какие конкретно параметры изменились.

Если модуль недоступен или ответ некорректен – лог предупреждения: “AI module unreachable, continuing with last known limits” или “AI provided out-of-range value, using cap X”. Это важно для отладки и подтверждения работоспособности механизма fail-safe.

Логи применения алгоритма: например, “Switched rate-limiting algorithm from FixedWindow to TokenBucket” при смене алгоритма, с указанием времени.

Логи fail-open: если случается отключение Redis (переход в fail-open), логируются события входа/выхода из режима fail-open, чтобы затем при анализе понимать, что в этот промежуток лимитер не работал и адаптивные настройки не применялись.

Метрики ИИ-модуля: Сам модуль Prophet может выставлять показатели качества прогноза (например, ошибку последнего прогноза) или время, оставшееся до следующего изменения. Косвенно эффективность можно оценивать, сравнивая predictedRps из присылаемых конфигураций с фактическим RPS (из метрик C/B). Если прогноз стабильно опережает события и настройки удачны – разница между предсказанным и реальным пиком небольшая, B не перегружен, 429% находится в разумных пределах.

Анализ постфактум: Для дипломного проекта предусмотрено сопоставление метрик и логов до/после внедрения адаптивного компонента. В отчётах по экспериментам будет отражено, как интеллектуальный модуль скорректировал лимиты перед всплесками и что из этого вышло. Например, в Scenario 2 (всплески нагрузки) планируется отметить, увеличил ли модуль лимит перед пиком или уменьшил, и как это повлияло на метрики B (см. логи и графики). Таким образом, ведение описанных выше метрик и логов напрямую поможет валидации: по ним можно количественно подтвердить, что адаптивное лимитирование либо улучшило пропускную способность без потери стабильности, либо повысило защищённость B при экстремальной нагрузке.

На основе совокупности этих данных будет сделан вывод об эффективности адаптивного подхода. Если метрики покажут более устойчивое поведение системы и лучшее сочетание пропускной способности vs. защита от перегрузок при включенном интеллектуальном модуле, то цель адаптации достигнута. In обратном случае, логи и метрики позволят pinpoint-проблемы (например, неточность прогноза или слишком резкие изменения лимита) для дальнейшего улучшения алгоритма.

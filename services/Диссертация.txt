Тема: Адаптивное управление нагрузкой в распределенной среде на основе интеллектуального анализа трафика


Введение
Во введении фиксируются:
актуальность темы исследования;
степень разработанности проблемы;
объект исследования — распределённые информационные системы;
предмет исследования — алгоритмы ограничения частоты запросов;
цель работы — исследование влияния алгоритмов RPS-лимитирования на устойчивость и защищённость распределённых систем;
задачи исследования (6–7 пунктов);
методы исследования;
практическая значимость;
структура работы.
Важно:
 Здесь один раз аккуратно указать:
В работе используется внешний интеллектуальный аналитический компонент для прогнозирования и классификации нагрузки без разработки собственных моделей искусственного интеллекта.

Глава 1. Теоретические основы устойчивости и защищённости распределённых информационных систем
1.1. Проблемы устойчивости и защищённости распределённых систем при высоконагруженном взаимодействии сервисов
Раскрываются:
понятия устойчивости, отказоустойчивости и надёжности;
информационная безопасность в распределённых системах;
характерные угрозы при высоком RPS;
примеры перегрузок, каскадных отказов, деградаций сервисов.
1.2. Роль ограничения частоты запросов как механизма обеспечения отказоустойчивости и предотвращения перегрузок
Рассматриваются:
причины необходимости RPS-лимитирования;
связь перегрузок с отказами и ретраями;
роль rate limiting в защите от злоупотреблений и DDoS;
место RPS-ограничений среди других защитных механизмов.
1.3. Особенности обеспечения устойчивости и защищённости публичных программных интерфейсов в распределённых системах
Анализируются:
специфика публичных API;
нагрузочные и репутационные риски;
практики API-провайдеров и API-маркетов;
требования к отказоустойчивости интерфейсов.

Глава 2. Алгоритмические и архитектурные подходы к реализации ограничений частоты запросов
2.1. Классификация алгоритмов ограничения частоты запросов: принципы, преимущества и ограничения
Подробно рассматриваются:
Fixed Window;
Sliding Window;
Leaky Bucket;
Token Bucket;
Сравнение проводится по критериям:
точность;
адаптивность;
сложность реализации;
применимость в распределённых системах.
2.2. Архитектурные модели интеграции механизмов ограничения запросов в распределённые системы
Описываются:
встраивание в приложение;
использование API-шлюзов;
централизованные сервисы лимитирования;
применение Redis и распределённых кэшей;
вопросы масштабирования и отказоустойчивости.
2.3. Компромиссы между точностью, отказоустойчивостью и адаптивностью механизмов ограничения запросов
Рассматриваются:
согласованность и доступность;
влияние CAP-теоремы;
конфликты репликации;
опыт промышленных решений и крупных API-провайдеров;
предпосылки к использованию адаптивных и интеллектуальных механизмов.

Глава 3. Экспериментальное исследование алгоритмов ограничения запросов в распределённой системе
3.1. Методика проведения эксперимента и описание архитектуры прототипа системы
Описываются:
цель и гипотезы эксперимента;
архитектура прототипа (сервисы A, B, балансировщик C);
используемые технологии (Java, Spring Boot, Redis);
сценарии нагрузки и типы трафика.
3.2. Реализация и сравнение алгоритмов ограничения запросов в условиях различных сценариев нагрузки
Рассматриваются:
реализация Token Bucket и Sliding Window;
настройка Redis и библиотек (Bucket4j / Resilience4j);
симуляция:
равномерной нагрузки;
всплесков;
злоупотреблений.
3.3. Использование внешнего интеллектуального аналитического компонента для адаптивного управления RPS
Описывается:
назначение интеллектуального компонента;
тип входных данных (метрики, временные ряды);
тип выходных рекомендаций (изменение лимитов);
место в архитектуре;
отсутствие обучения и разработки ИИ в рамках работы.
3.4. Оценка устойчивости и защищённости системы на основе экспериментальных данных
Анализируются:
процент отклонённых избыточных запросов;
среднее время отклика;
поведение системы при пиковых нагрузках;
устойчивость к злоупотреблениям;
влияние адаптивного управления на стабильность системы.

Заключение
Включает:
основные выводы по каждой главе;
подтверждение достижения цели и задач;
обобщение результатов эксперимента;
оценку эффективности адаптивного подхода;
ограничения исследования;
перспективы дальнейшей работы.

Список использованных источников
Приложения
(архитектурные схемы, конфигурации, фрагменты кода, сценарии нагрузки)

Глава 1. Теоретические основы устойчивости и защищённости распределённых информационных систем

1.1. Проблемы устойчивости и защищённости в распределённых системах при высоконагруженном взаимодействии сервисов

Отказоустойчивость в распределённых системах – это способность системы сохранять надёжность (доступность и устойчивость) при отказах отдельных компонентов или сбоях в подсистемах [22]. Информационная безопасность означает защиту информации и поддерживающей её инфраструктуры от случайных или преднамеренных воздействий (природных или антропогенных), которые могут нанести вред владельцам или пользователям системы [24]. В распределённых системах эти понятия тесно связаны, так как надёжность требует сохранения работоспособности при больших нагрузках, а безопасность – сохранения конфиденциальности, целостности и доступности данных и сервисов. Нельзя просто взять и свести безопасность только к защите конфиденциальности, ведь сбои в обслуживании (например, падение сервиса под нагрузкой) также нарушают безопасность системы [24].
Предположим, что интерфейс в кластере A обрабатывает 1000 запросов в секунду (QPS), как показано на рисунке 1:

Если кластер B выходит из строя (рис. 1), количество запросов к кластеру A увеличивается до 1200 запросов в секунду. Фронтенды в кластере A не могут обрабатывать запросы со скоростью 1200 запросов в секунду и поэтому начинают испытывать нехватку ресурсов, что приводит к сбоям, срыву сроков или другим проблемам. В результате количество успешно обработанных запросов в кластере A значительно снижается и составляет менее 1000 запросов в секунду [49].

Такие каскадные отказы приводят к тому, что локальный сбой быстро охватывает всю систему. Например, при отказе одного сервиса микросервисы могут заполнить очереди запросами, исчерпать ресурсы и сами упасть. Как отмечают эксперты, наиболее распространённой причиной цепных сбоев является именно перегрузка компонентов: они длительное время обрабатывают растущую очередь запросов и в итоге выходят из строя [49]. Это особенно актуально для высоконагруженных сервисов, обрабатывающих тысячи запросов в секунду.
К ключевым угрозам устойчивости и безопасности распределённых систем при высоких нагрузках относятся:
DDoS-атаки и перегрузка сервисов: злоумышленник или случайный всплеск трафика может накачать запросами систему, делая её недоступной или крайне медленной. По данным индустрии, DDoS-атаки остаются «угрозой номер один» благодаря своей эффективности и простоте реализации. Современные Layer 7 (прикладные) DDoS-атаки особенно опасны в микросервисных архитектурах – даже малые объёмы скоординированного трафика могут остановить публичные API и порталы. К примеру, в 2018 г. GitHub пережил DDoS с пиковыми 1,35 Tbps трафика, который был организован с помощью открытой уязвимости memcached. Такие атаки замедляют или полностью останавливают сервисы, уменьшают доверие пользователей и могут стоить бизнесу миллионов долларов [7].

Каскадные отказы и исчерпание ресурсов: при росте нагрузки один сервис может начать сбои, и оставшиеся узлы берут на себя лишнюю работу. Так возникает эффект домино: например, заполнение связанных друг с другом пулов или исчерпание памяти у падающего сервиса ведёт к его падению, что затем увеличивает нагрузку на другие компоненты. В реальном мире описаны случаи, когда незаметный отказ (например, сбой DNS) запускал цепную реакцию по всему кластеру. Так, в октябре 2025 г. простая ошибка в AWS US-EAST-1 (отказ DNS у сервиса DynamoDB) вызвала каскадный сбой 75 отдельных сервисов (EC2, S3, RDS и др.) по всему облаку. Это привело к 15-часовому простою для тысяч компаний, показав, насколько распределённая инфраструктура уязвима к таким сценариям [47].
Уязвимости ПО и ошибки конфигурации: каждый компонент системы может содержать баги (SQL-инъекции, ошибки управления памятью и т.д.), через которые атакующий получает доступ или вызывает сбой. В микросервисах зачастую десятки сервисов используют разные языки, базы и библиотеки, поэтому непростая задача поддерживать их все в актуальном и безопасном состоянии. Некорректные настройки сервисов или отсутствие нужных лимитов – частые причины проблем. Например, описан кейс потокового видео-сервиса, у которого рекомендательный микросервис не имел лимитов запросов: бот-трафик довёл его CPU до 100%, что вызвало полный отказ рекомендаций, который начал распространяться на зависимые сервисы. Другой пример – злоупотребление автоскейлингом без лимитов: под DDos-сбой кластер может автоматически нарастить сотни узлов, а затем выставить компании счет в сотни тысяч долларов (экономический DoS) [27].
Нарушение аутентификации и контроля доступа: если проверка прав доступа реализована слабо, злоумышленник может подменить пользователя, вывести закрытые API на публичный доступ или получить учётные данные. Слабые пароли, отсутствие двухфакторной аутентификации или ошибки в механизмах токенов создают брешь в системе безопасности [18]. Уже взломав аккаунт, злоумышленник может от лица легитимного сервиса генерировать множество запросов, и если нет ограничения скорости на пользователя, это практически неотличимо от обычной активности (проблема OWASP API, сбой Business Logic Flow).
Перехват и модификация данных: распределённые сервисы обмениваются данными по сети. Если трафик не защищён (например, без TLS или слабо аутентифицирован), атакующий может перехватить запросы и изменить или прочитать конфиденциальную информацию [18]. Это особенно опасно при высоких нагрузках, когда взаимодействуют несколько сервисов, а этот процесс не прозрачен. Кроме того, злоумышленник может внедрить вредоносное ПО или использовать уязвимости в сторонних сервисах (supply chain) для получения доступа к критическим данным.
Человеческий фактор и инсайдеры: ошибки администраторов (размещение неотладочного кода в продакшн, неверная сетка политики, забытые ключи и т.д.) и недобросовестные сотрудники могут стать причиной масштабных проблем. В микросервисной архитектуре эксперты отмечают, что для обеспечения безопасности нужно отслеживать огромное число взаимодействий, а этого часто не делают [27]. Человеческие ошибки могут непреднамеренно открыть внутренние сервисы или отключить защитные механизмы, усугубив нагрузку на систему.
Таким образом, при проектировании распределённых систем под высокую нагрузку важно одновременно обеспечивать устойчивость (наличие резервных компонентов, автоматическое перераспределение нагрузки, проверку корректности работы) и защищённость (шифрование каналов, жёсткая аутентификация, мониторинг аномалий). В реальных условиях сбои и атаки неизбежны – поэтому системы проектируют с избыточностью и механизмами быстрого обнаружения и смягчения последствий (лимитирование запросов, схема Circuit Breaker, CAP-модель и др.), чтобы предотвратить описанные угрозы [7] [27].

1.2. Роль ограничения частоты запросов как механизма обеспечения отказоустойчивости и предотвращения перегрузок

Ограничение скорости запросов (rate limiting) – один из ключевых механизмов отказоустойчивости и защиты сервисов от перегрузок. Оно устанавливает порог на число запросов за единицу времени (обычно в RPS) и устраняет избыточный трафик, не позволяя системе превысить установленную пропускную способность. Это важно как при случайных всплесках трафика (например, акции продаж, появление вирусного контента или ошибки в коде клиентов), так и при преднамеренных атаках (DDoS, брутфорс, флуда). Ограничители RPS позволяют сохранять стабильность сервиса, перераспределяя ресурсы и обеспечивая стабильное обслуживание запросов [38].
Защита от пиковых нагрузок. Неожиданные скачки трафика (распродажи, многочисленные запросы от ботов) могут мгновенно привести к перегрузке сервиса. Rate limiting убирает часть входящих запросов, позволяя обработать часть допустимой нагрузки. Например, в Habr описаны ситуации: «знаменитость опубликовала важный пост, и куча людей кинулась его читать; или компания устроила распродажу, и покупатели обрывали сайт; или началась DDoS атака». Механизм RPS-лимита эффективно сглаживает такие пики, просто отклоняя избыток запросов [23].
Предотвращение лавины повторов. При сбоях или задержках отдельного сервиса клиенты часто начинают повторять запросы, что может вызвать каскадный сбой всей системы. Без ограничений такая лавина запросов перегружала сеть. Например, в сбое Amazon 2021 года из-за ошибки на внутреннем сервисе сработал автоматический повтор запросов без rate-limit, и компании сами себя задедосила – шквал повторных запросов привёл к остановке Amazon, Slack, Netflix и др. Ограничитель RPS прерывает подобный сценарий, подавляя лавину: сервис будет обрабатывать только заранее допустимое число запросов, даже при массовом ретрае. С технической точки зрения, rate limiting дополняет паттерн Retry (экспоненциальный бэкофф, случайный джиттер) и Circuit Breaker: если приходит слишком много запросов – сервис возвращает «429 Too Many Requests» и указывает заголовок Retry-After, предлагая клиентам подождать [12] [23]. Это помогает избежать усугубления исходной проблемы и даёт системе время восстановиться.
Защита от злоумышленников и ботов. Ограничение RPS – первая линия обороны против целенаправленных атак. Оно позволяет задать порог запросов для каждого клиента (IP или аккаунта), не давая злоумышленнику «обвалить» систему простым флудом. «Цель ограничения скорости — не дать злоумышленнику перегрузить веб-приложение большим количеством запросов, вызвав сбой сервера». Например, чтобы предотвратить DDoS-атаки прикладного уровня, используют правила ограничения RPS на брандмауэрах и балансировщиках: после достижения лимита все дополнительные запросы отклоняются или задерживаются [33]. Помимо DDoS, RPS-лимит эффективно блокирует брутфорс (повторные попытки взлома паролей) и спам-запросы, распределяя ресурсы равномерно среди обычных клиентов [20] [38].
Стабильность и равномерность нагрузки. Кроме явной защиты от «волн» трафика, rate limiting обеспечивает плавную обработку потока запросов (back-pressure). При управлении потоками запросов в сети такие лимиты помогают избежать «ресурсного голодания» отдельных сервисов. В паттерне Rate Limiter ограничивается скорость поступления, чтобы обеспечить стабильность, отзывчивость и доступность системы при меняющемся трафике [9]. На целевой сервис попадёт только столько запросов, сколько не приведёт к дефициту ресурсов [20].
Примеры из практики и реализации
Промежуточное ПО и шлюзы (API Gateway). Ограничение скорости часто реализуют на уровне API-шлюзов и прокси: NGINX, Kong, AWS API Gateway, Azure API Management и др. Например, в NGINX есть модули ngx_http_limit_req и ngx_http_limit_conn (используют алгоритмы Token Bucket) для отсечения избыточного трафика [33]. Фреймворки и библиотеки (Spring Cloud Gateway, Envoy, Express.js) также имеют встроенные фильтры/мидлвары для rate limiting. Так, в Spring Cloud Gateway есть фильтр RequestRateLimiter, который с помощью токен-бакета искусственно ограничивает RPS и защищает сервис от перегрузок и брутфорса [20]. Платформы микросервисов и сервис-меши (Istio, Envoy, Linkerd) предоставляют подобные возможности на «границе» сервисов.
Веб-серверы и сети доставки контента. На уровне веб-серверов и CDN ограничение RPS тоже широко применяется. Apache HTTPD имеет модули mod_qos, mod_evasive и др. для контроля количества соединений и запросов. Облачные CDN/Firewall-сервисы (Cloudflare, Akamai, Yandex.Cloud WAF) умеют фильтровать трафик и накладывать глобальные rate-лимиты на клиентов по IP или сегменту сети [33] [38]. Это снижает нагрузку на исходные сервисы и смягчает DDoS-атаки на уровне доставки контента.
Облачные решения и WAF. Облачные платформы включают встроенные средства RPS-лимитирования в свои сервисы безопасности. Например, AWS WAF поддерживает rate-based rules – блокирующие правила по превышению RPS. AWS Shield и балансировщики (ELB/ALB) умеют автоматически ограничивать клиентов, «превышающих заранее заданные пороги». Аналогично, Azure Application Gateway/WAF и Azure Front Door могут ставить лимиты на входящий трафик, а Google Cloud Armor – блокировать IP при превышении RPS [33]. Эти инструменты позволяют задать централизованную политику защиты от HTTP-флуда и DDoS, сохраняя доступность сервиса для обычных пользователей.
Ограничение количества запросов – достаточно простая, но эффективная мера для повышения отказоустойчивости приложений. Как показывает практика, без RPS-лимитов сервисы рискуют перегрузиться при пиковом трафике, впасть в метастабильное состояние или обрушиться из‑за каскадных повторов и DDoS [9]. Реализация rate limiting (например, через токен-бакет или скользящее окно) устраняет возможность дойти лишним запросам до критических компонентов, а система в свою очередь продолжает функционировать в пределах своих возможностей [20] [23]. В сочетании с другими паттернами (экспоненциальный бэкофф, джиттер, circuit breaker) это обеспечивает плавное обслуживание нагрузки и устойчивость к аномалиям. При этом необходимо тщательно настраивать лимиты, чтобы не блокировать обычный трафик: специалисты рекомендуют мониторить события rate limiting и корректировать пороги [23] [33].

1.3. Особенности обеспечения устойчивости и защищённости публичных программных интерфейсов в распределённых системах

Публичный API – это интерфейс, предоставляемый внешним разработчикам для интеграции с сервисом. Такие API должны быть высокодоступными и стабильными. Ключевые требования к публичному API включают долгосрочную стабильность версий и предсказуемость поведения, а также полное документирование всех эндпоинтов (например, через OpenAPI). Для обеспечения устойчивости системы API-поставщик часто предоставляет портал разработчика (self-service), где клиенты могут самостоятельно получать ключи, изучать документацию и подписываться на сервис. Это позволяет сервису обслуживать тысячи клиентов и масштабироваться под нагрузку: в частности, в примере массового API для интеграторов отмечается необходимость «поддержки тысяч клиентов с разными потребностями». Для обратной совместимости критично версионирование – например, URI-путями (/v1/…, /v2/…) или заголовками – что обеспечивает ясность и длительную совместимость API. Поскольку публичные API ориентированы на внешних пользователей, они обычно требуют автоматической генерации SDK и примеров из спецификаций (OpenAPI), что упрощает подключение сторонних разработчиков [44].
Доступность и масштабируемость: Сервисы управления API (например, AWS API Gateway или Azure API Management) развёрнуты в нескольких зонах доступности и регионах, автоматически восстанавливаются при сбоях одного узла и масштабируются по необходимости [11] [32]. Такие платформы обеспечивают резервирование и балансировку нагрузки из коробки.
Документированность: Публичные API сопровождаются полным описанием интерфейса (спецификацией), примерами запросов и ответов. Это снижает когнитивную нагрузку на разработчиков и повышает доверие к API [44].
Версионирование: Обновления API должны быть совместимы с предыдущими версиями. Например, URI-версии (/v1/…) позволяют чётко отделять новые релизы, сохраняя работу старых интеграций [44].
Self-service: Поставщик API организует процедуру самостоятельного подключения клиентов – регистрация приложения, получение ключей/токенов, настройка квот – чтобы минимизировать участие разработчиков API в процессе интеграции.
Прозрачность ошибок: В публичных API ошибки обрабатываются предсказуемо – используются стандартные HTTP-коды (например, 400, 401, 429, 500) и структурированные сообщения. Код 429 Too Many Requests с заголовком Retry-After сигнализирует о превышении лимита запросов. При этом тела ошибок не должны содержать внутренних деталей (стек-трейсы, пути файлов, конфиденциальные данные) из соображений безопасности [44]. Это повышает доверие к API и облегчает отладку на стороне клиента.

Нагрузочные риски публичных API
Публичные API особенно уязвимы к нагрузочным рискам. Во-первых, возможны непредсказуемые всплески трафика (например, резкий рост числа запросов при маркетинговой акции или ошибке интеграции клиента). Без адекватного контроля такие пики могут перегрузить сервис. Во-вторых, внешние системы могут подвергаться DDoS-атакам: специально сгенерированный поток запросов способен исчерпать ресурсы (CPU, память) и привести к отказу сервиса. OWASP-категория API4 («Unrestricted Resource Consumption») описывает этот риск как «неограниченное потребление ресурсов API может привести к отказу в обслуживании или росту операционных затрат» [37]. Также неэффективные или buggy-клиенты сами могут неосознанно вызывать лавинообразный рост запросов (например, из-за зацикливания или отсутствия кэширования), что тоже подвергает систему риску. Механизм rate limiting (throttling) становится здесь важным элементом защиты: при превышении квоты API Gateway автоматически отклоняет избыточные запросы с кодом 429. Это, с одной стороны, защищает от захвата ресурсов недобросовестными клиентами или ошибками интеграторов, а с другой – ограничивает эффективность DDoS (хотя полностью от таких атак rate limiting не спасает). В целом для устойчивости системы рекомендуют сочетать защиту от перегрузок (throttling, квоты, backpressure) с масштабированием (автоскейлинг) и мониторингом производительности [39].
Непредсказуемые пики: В моменты высокой нагрузки (например, популяризация API) без масштабирования и ограничений API может упасть или значительно замедлиться.
DDoS-атаки: Атакующие могут целенаправленно посылать огромный поток запросов. Rate limiting лишь частично снижает эффект (оно отбрасывает трафик сверх квоты, но сетевая нагрузка при этом всё равно создаётся) [39]. Для защиты часто применяют шлюзы и CDN с масштабируемой пропускной способностью.
Неэффективные клиенты: Ошибки на стороне интеграции (например, бесконечные повторные запросы из-за багов) тоже создают дополнительную нагрузку. Правильно настроенные лимиты позволяют быстро отсекать такие случаи, не давая им вывести сервис из строя [39].
Злоупотребления: Ключи API могут быть украдены или скомпрометированы, и злоумышленник может использовать их для массовых запросов. Помимо rate limiting, рекомендуется отслеживать аномальную активность и иметь возможность быстро деактивировать подозрительный ключ.

Механизмы защиты публичных API
Безопасность публичных API обеспечивается в первую очередь через идентификацию и авторизацию клиентов. Аутентификация (например, через OAuth 2.0, JWT-токены или API-ключи) устанавливает личность вызывающего, а авторизация (RBAC/ABAC) – проверяет его права на конкретные операции. В индустрии OAuth 2.0 признан стандартным протоколом авторизации для API [36]. Правильно настроенные OAuth-флоу позволяют выдавать клиентам минимально необходимые по области действия (scope) токены. Нарушение механизмов аутентификации – частая уязвимость (OWASP API Security API2: «Broken Authentication»), когда атака позволяет красть токены или подменять пользователя [37]. Не менее важно разграничивать уровни прав: например, даже аутентифицированный пользователь не должен иметь доступ к чужим ресурсам (никакой «утечки» через ошибочную авторизацию). API часто раскрывают внутренние идентификаторы объектов, и без тщательных проверок уровня доступа это приводит к уязвимостям (API1: «Broken Object Level Authorization») [37]. Следует обеспечить принцип наименьших привилегий и проверять права на каждую критическую операцию.
Для предотвращения спама и недобросовестного поведения пользователей используются фильтры и антибот-защита, а также уже упомянутое ограничение частоты запросов. Обработка ошибок также играет роль безопасности: в продакшн-окружении тело ответа не должно содержать отладочной информации (стек-трейсы, SQL-запросы и т.п.), чтобы случайный или целенаправленный наблюдатель не получал подсказок о внутреннем устройстве системы. Это снижает риски инъекций и иных атак. К тому же стоит логировать все критические ошибки со служебными trace-id – чтобы при инцидентах можно было отследить источник без раскрытия подробностей клиентам [44].

Открытость API и потенциальные уязвимости
Хоть открытость API и обеспечивает гибкую интеграцию, она несёт свои риски. Публичные API по определению раскрывают логику сервиса и доступ к данным, обнародуют критические бизнес-функции и PII (личные данные), что создаёт широкое поле для атаки [37]. По сути, любое смежное приложение или бот получает детальное описание интерфейса, по которому может попытаться найти уязвимости. В отчёте World Wide Technology говорится, что расширение API нередко приводит к «увеличению поверхности атаки, экспонированию бизнес-логики и/или утечке данных» [48].
Кроме того, публичный API может стать посредником для автоматических атак на бизнес-логику. Например, злоумышленник может программно воспроизводить легитимные сценарии (выполнение операций, перевод денег и т.д.) в обход обычных фронтендов, используя определённый паттерн поведения. Если такая возможность не учтена (например, не введены квоты на бизнес-события), то это рассматривается как одна из рисков OWASP (API6: «Unrestricted Access to Sensitive Business Flows»). Неконтролируемые или избыточные запросы к API напрямую затрагивают доступность системы: при отсутствии ограничений на потребление ресурсов (OWASP API4) даже честный, но при этом жадный клиент может вызвать DoS [37]. Таким образом, открытость API диктует необходимость дополнительного контроля – тщательной фильтрации входящих вызовов, ограничения скорости и глубины запросов, а также регулярного обзора кода на предмет уязвимостей.

Лучшие практики и стандарты API-безопасности
Существует ряд признанных стандартов и практик, призванных минимизировать перечисленные риски. Ключевым справочником в сфере безопасности API является OWASP API Security Top 10, где перечислены типичные уязвимости (от неправильной аутентификации до неограниченного потребления ресурсов) [37]. Среди технических мер стоит отметить:
OAuth 2.0 / OpenID Connect – современные фреймворки для авторизации и аутентификации клиентов [36]. Их использование стандартизирует процесс выдачи токенов и разграничения доступа.
Rate limiting и quotas – разграничение скорости и объёма запросов по ключам/IP. Как описано ранее, это предотвращает перегрузки и частично DDoS [39].
CORS-политики – настройка заголовков Access-Control-Allow-Origin и др. для контроля доменов, с которых разрешены запросы. Неправильная конфигурация (например, открытие доступа * при использовании учётных данных) может привести к утечкам чувствительных данных [37]. В общем случае следует строго белый список разрешённых источников либо полностью открывать CORS только для действительно публичных безопасных данных.
API-ключи и токены – управление ключами включает ограничение их прав (scope), быструю ротацию при компрометации и надёжное хранение на клиенте. Например, документация RapidAPI рекомендует ротацию ключей: при подозрении на утечку можно «перезапустить» ключ без потери аналитики сервиса [40].
Мониторинг и аудит – логирование запросов и метрик (CPU, латентности, статусы ответов) позволяет вовремя обнаружить аномалии. Рекомендуется устанавливать алёрты на нештатные паттерны (внезапный рост числа запросов, большое число ошибок 5xx и т.д.).
Таким образом, интеграция API-гейта (шлюза) или WAF, совместно с проверенными стандартами (OAuth, JWT, API Keys, SSL/TLS), и регулярное применение обновлений и патчей платформ, являются лучшими практиками.
Роль API-маркетов и платформ в надёжности и безопасности систем.
Современные облачные платформы и API-маркеты значительно упрощают развертывание публичных интерфейсов с точки зрения надёжности. Например, Amazon API Gateway – полностью управляемый сервис, автоматически распределяющий запросы по нескольким зонам доступности для высокой отказоустойчивости. По умолчанию он накладывает глобальные лимиты (steady-state и burst) на вызовы, предотвращая перегрузку без дополнительной настройки [11]. Аналогично Azure API Management позволяет настраивать несколько единиц (unit) обработки запросов, распределённых по зонам и регионам, а также задавать политики повторов, rate-limit’ов и квот на уровне API [32]. Такой подход обеспечивает, что в случае отказа одной ноды или даже целого ЦОДа остальные продолжат работу.
Кроме чисто технической инфраструктуры, существуют и коммерческие API-маркеты, которые помогают распространению публичных API. RapidAPI – один из крупнейших API – предоставляет единую площадку с поддержкой OAuth2, API-ключей, аналитики и ограничений. Например, разработчик может опубликовать свой API в RapidAPI, задать тарифные планы и квоты, а подписавшиеся клиенты получат доступ без дополнительных согласований. В документации RapidAPI подчёркивается возможность ротации ключей без прерывания работы: если ключ скомпрометирован, его можно быстро заменить на новый [40]. Это демонстрирует подход «безопасность при разработке» (security by design). Аналогичные возможности предлагают и другие платформы: Google Cloud Marketplace/API Gateway (Apigee), Azure Marketplace и др. Они включают встроенную аутентификацию, SSL-шифрование, мониторинг и защиту на уровне гипервизора. В результате роль этих платформ – унифицировать применение мер безопасности и масштабирования из коробки, освобождая разработчиков от большинства рутинных задач. Используя их, организации добиваются достаточно высокой надёжности системы: отказ любого компонента собственной разработки быстро компенсируется уменьшением затрачиваемых человеко-часов, а встроенные механизмы авторизации и фильтрации предотвращают многие типовые атаки.
Публичные API в распределённых системах должны сочетать открытость и гибкость с жёсткими механизмами контроля. Характеристики API (документированность, стабильность версий, масштабируемость) определяют удобство интеграции и стабильность сервиса, а правильно настроенные политики безопасности (аутентификация, авторизация, rate-limiting, контроль ошибок) защищают как от перегрузок, так и от злоумышленных действий. Современные стандарты и платформы (OAuth2, OWASP API Top 10, API Gateway, API-маркеты) составляют практическую основу для реализации этих принципов [12] [37] [44].

Глава 2. Алгоритмические и архитектурные подходы к реализации ограничений частоты запросов

2.1. Классификация алгоритмов ограничения частоты запросов: принципы, преимущества и ограничения

Алгоритмы ограничения частоты запросов контролируют число входящих запросов за единицу времени, которые при этом защищают систему от перегрузок и атак. К ним относятся такие подходы как: фиксированное окно (Fixed Window), скользящее окно (Sliding Window), протекающее ведро (Leaky Bucket) и токен-ведро (Token Bucket). Все они различаются по подсчёту запросов, реакции на всплески и сложности реализации [16] [41]. Рассмотрим их работу, поведение при разной нагрузке и преимущества/ограничения.
Fixed Window (фиксированное окно)
При фиксированном окне время делится на равные отрезки (например, по минутам), и ведётся простой счётчик запросов в каждом окне [17] [41]. Если число запросов превысило порог внутри окна, последующие запросы блокируются до начала следующего окна.
Принцип работы: завести счётчик count и метку начала окна t0. Для каждого запроса проверять, не закончился ли текущий интервал (если now - t0 >= окно – сброс счётчика). Если count < лимит, разрешить запрос и увеличить count, иначе – отклонить.
Поведение при нагрузках: при равномерной нагрузке алгоритм жёстко ограничивает пропускную способность на уровне лимита. При резких всплесках может возникнуть двоение: запросы на границе двух окон суммируются, что позволяет кратковременно превысить среднюю скорость [8] [17]. Например, если пришло максимальное количество запросов за последние доли секунды одного окна и начало следующего, суммарная нагрузка временно вдвое превысит лимит.
Преимущества: очень прост в реализации и высокопроизводителен (достаточно атомарного инкремента счётчика). Хорошо подходит для равномерного потока запросов.
Недостатки: низкая точность контроля и нечувствительность к изменению трафика. Может пропускать пики на границах окон. Не адаптивен: порог фиксирован и не меняется при изменении паттернов.
Во многих библиотеках rate limiting по умолчанию — это и есть fixed window:
Express / Node.js: ранние и самые простые middleware типа express-rate-limit из коробки работали фактически как fixed window: счётчик запросов на IP в текущем интервале (например, 1 минута), затем обнуление. Сейчас в доках часто рекомендуют sliding window / token bucket, но простой режим всё ещё доступен.
Django / DRF и другие веб-фреймворки (Python, PHP, Ruby): самодельные решения через Redis INCR + TTL — тоже fixed window (ключ "user:{id}:minute:{timestamp}", живёт минуту, внутри просто счётчик).
Многие managed API-шлюзы и SaaS-сервисы исторически начинали именно с fixed window:
Early versions rate limiting в некоторых API-gateway (самописных): конфиг вида «1000 запросов в минуту» без упоминания sliding — почти всегда реализация через fixed window counter.
В некоторых SaaS-платформах (разные маркетинговые/CRM/API-сервисы) дешёвые тарифы могут использовать простой fixed window, а на дорогих — более совершенные алгоритмы (sliding window / token bucket).
У больших игроков (AWS API Gateway, Cloudflare, Kong и т.п.) сейчас под капотом чаще смешанные/скользящие алгоритмы.
Внутри компаний fixed window часто используется там, где:
требуется грубое, но простое ограничение,
нет жёстких требований к fairness,
важна предсказуемость и простота реализации.
Например:
Ограничение количества:
логинов / попыток авторизации с IP в минуту;
запросов к сервису отчётности ;
внутренних интеграций по типу «не больше 100 запросов в ERP в минуту с одного сервиса».
Ограничения для фоновых задач: «не больше N внешних API-вызовов в минуту в сторону партнёра» — внедряется просто счётчиком за минуту.
Sliding Window (скользящее окно)
Скользящее окно повышает точность фиксированного подхода, разделяя время на более мелкие отрезки и двигая окно с течением времени. Один из вариантов – «скользящее окно со счётчиком» (sliding-window counter): хранить два прошлых и текущий счётчики для окон меньшей длины. Более точный метод – sliding-window log: сохранять метки времени всех запросов в списке или упорядоченном множестве и удалять устаревшие [8] [17].
Принцип работы: при поступлении запроса освобождаются (удаляются) отметки запросов старше заданного периода (например, 1 минута). Если оставшихся запросов меньше лимита, запрос разрешается и его отметка добавляется в список. Иначе – отклоняется. Таким образом окно «скользит» вдоль временной шкалы.
Поведение при нагрузках: этот алгоритм более гладко реагирует на переменный трафик. При равномерном потоке позволяет выдерживать лимит без скачков, а при всплесках ограничивает их более плавно – не происходит резкого удвоения из-за границы окна. Однако любые превышения лимита всё равно приводят к отклонению запросов.
Преимущества: высокая точность контроля и гибкость: уровень ограничения можно тонко настраивать через длину окна и частоту пересчёта. Алгоритм масштабируем и может комбинироваться с другими подходами.
Недостатки: высокие накладные расходы и сложность реализации. Необходимо хранить историю меток запросов (память и CPU-рассходы). При очень бурном трафике в короткие интервалы алгоритм не допускает сбора пиковых запросов – они отклоняются (что может привести к потерям при обработке резких всплесков). В распределённой среде реализация требует синхронизации меток между узлами (например, с помощью Redis sorted set), что ещё больше усложняет систему.
На практике можно встретить скользящее окно в следующих системах:
1. API-шлюзы и сервис-меши (современный «стандарт»)
Во современных API-gateway / service mesh чаще всего встречаются либо Sliding Window, либо Token Bucket, иногда гибридный вариант – Envoy / Istio-подобные решения — часто используют модели, эквивалентные sliding window или token bucket, чтобы не было всплесков запросов на границе минут.
2. Публичные SaaS-API с жёсткими лимитами
Многие крупные SaaS-сервисы и API-платформы, где много клиентов, важна справедливость и предсказуемость, и при этом лимиты жёстко задекларированы, используют Sliding Window Log:
каждый запрос записывается с timestamp (часто в Redis sorted set / БД in-memory);
при каждом новом запросе удаляются старые записи старше окна T;
если количество записей в окне ≥ лимита — 429.
Или Sliding Window Counter (двойное окно):
хранит два счётчика: для текущего и предыдущего окна;
лимит считают как w_prev * (остаток времени) + w_current, получается плавное окно.
Такие схемы типичны для:
провайдеров платёжных API,
маркетинговых / e-mail / SMS-платформ,
big-tech API (когда в доках честно пишут про «rolling window»).
3. Клиентские SDK / мобильные / десктопные клиенты
Когда клиентское приложение обязано не превышать лимиты стороннего API, в SDK часто реализуют либо встроенный token bucket, либо sliding window.
Token Bucket (алгоритм токен-ведро)
В алгоритме токен-ведра имеется пул (макс. число токенов) и постоянная скорость пополнения токенов. Каждый входящий запрос рассходует один токен: если токен есть в ведре, разрешаем запрос и удаляем токен, иначе – блокируем. Токены добавляются с заданной скоростью (например, 5 токенов в секунду), пока ёмкость не заполнится [8] [41].
Принцип работы: поддерживаем счётчик токенов tokens. В фоновом режиме (таймером или атомарной операцией) добавляем новые токены с заданной скоростью вплоть до максимальной ёмкости. При запросе проверяем: если tokens > 0, то tokens-- и запрос идёт, иначе – запрос отклоняется. Таким образом система накапливает токены в периоды простоя и расходует их при всплесках.
Поведение при нагрузках: при равномерной нагрузке система стабилизируется на заданной скорости обработки, поскольку токены приходят и расходуются с примерно одинаковой скоростью. При всплесках алгоритм допускает их погашение за счёт накопленных токенов – в короткий момент может пропустить запросы сверх среднего лимита, пока в ведре не кончатся токены [17] [41]. Если же токенов не хватает (например, при длительной атаке), дальнейшие запросы блокируются. Т.е. алгоритм гасит пики до выработки резерва.
Преимущества: сочетает точность ограничения с возможностью кратковременных перегрузок. Поддерживает адаптивность: при спадах нагрузки токенов накапливается больше, что позволяет принять больше запросов в последующем. Алгоритм широко используется на практике (Java-библиотека Bucket4j основана на нём [16]).
Недостатки: относительно высокая сложность реализации по сравнению с фиксированным окном. Требуется синхронный механизм пополнения токенов (шедулер или атомарные операции с базой) [8]. При очень резких, коротких всплесках все токены могут быстро израсходоваться, после чего система мгновенно начнёт блокировать запросы, другими словами токен-ведро подходит для всплесков, но истощается при постоянной атаке. Кроме того, нужно точно определить время для пополнения токенов, что возможно в том случае, если мы точно знаем критическую нагрузку, которую может обработать сервис и какая у него средняя нагрузка.
Обычно встречается в:
Сетевых устройствах и QoS:
маршрутизаторы, свитчи, шейперы трафика (Cisco, Juniper и т.п.);
Linux tc (traffic control) и похожие утилиты — классическое применение для регулировки пропускной способности каналов.
Облачных API и API-gateway’ях
где предполагаются лимиты, например, 100 rps в среднем, но с короткими всплесками до 200
AWS / GCP / Cloudflare / NGINX / Envoy и др. используют именно такой механизм для сочетания среднего лимита и пиковых всплесков.
Клиентских SDK и мобильных/desktop-приложениях
встроенные лимитеры для вызовов внешних API (карты, аналитика, переводы и т.д.), чтобы не превышать квоты;
локальные библиотеки ретраев + rate limiting — часто именно token bucket, чтобы не подавлять редкие всплески после переподключения.
Внутренние микросервисы с небольшим диапазоном лимитов
когда одному сервису нужно защититься от других, но при этом не запрещать короткое всплески запросов (например, батч-операции, миграции, разовые тяжёлые запросы от админок).
Leaky Bucket (алгоритм протекающего ведра)
В алгоритме протекающего ведра входящие запросы помещаются в очередь (ведро), из которой они обрабатываются с фиксированной скоростью [8] [35]. Вода (запросы) заливается в ведро произвольно, а вытекает строго равномерно как бы через дыру – пока ведро не переполнится.
Принцип работы: аналогия с реальным ведром: при поступлении запроса проверяем, есть ли свободное место в ведре (текущая очередь меньше ёмкости). Если да, ставим запрос в очередь. Параллельно запускается таймер (или вычисление), который как бы сливает из ведра текущие запросы – например, каждую миллисекунду разрешается один запрос или освобождается определённый объём. Если при поступлении запросов ведро заполнено – новые запросы отбрасываются. Таким образом система выравнивает поток по фиксированной скорости.
Поведение при нагрузках: при равномерной нагрузке алгоритм пропускает запросы тем же устойчивым темпом (определяется скоростью утечки). При всплесках лишние запросы накапливаются в очереди (в буфере) до ёмкости; если очередь переполнилась – наиболее поздние запросы отбрасываются. Это значит, что пиковые нагрузки сглаживаются по заданной норме, и резкие всплески не приводят к резкому падению производительности, но часть запросов может быть задержана или потеряна. Алгоритм менее гибок по сравнению с токен-бакетом: он не позволяет кратковременно увеличить пропускную способность выше установленного потока.
Преимущества: даёт плавный выходной поток запросов и предотвращает падение сервиса (flash crowd). Контролирует скорость обработки так, чтобы не перегружать систему [8]. Можно гибко выбирать размеры «ёмкости» ведра и скорость утечки под разные сценарии. Алгоритм прост по концепции и реализуется, например, в NGINX для защиты от пиков [35].
Недостатки: требует дополнительной памяти/буфера для очереди (накладные расходы). При очень резких всплесках часть запросов неизбежно будет отброшена. Алгоритм не приспособлен к быстро меняющемуся профилю трафика – он всегда обрабатывает очередь с постоянной скоростью, чего не всегда достаточно для оптимальной работы современных веб-приложений  [8].
Leaky Bucket используют там, где нужно жёстко сгладить поток и выдавать запросы почти с фиксированной скоростью, не пропуская продолжительные всплески запросов.
Встречается в следующих областях:
Сетевые шейперы и телеком
оборудование и софт, который выравнивает трафик: пакеты наливаются в ведро рывками, а вытекают с постоянной скоростью;
схемы трафик-шейпинга, когда важна предсказуемая полоса, а не гибкость резких всплесков.
Очереди запросов в backend’ах
когда сервис получает поток рывками, но обязан обрабатывать почти линейным темпом (например, лимит на запросы к медленному внешнему API);
реализация через очередь + фиксированный rate обработки — это по сути leaky bucket.
Очереди задач / job workers
диспатчер задач «сливает» задания в воркеры с фиксированным темпом, даже если в инпуте был пик;
полезно, когда важно не перегреть downstream-систему и держать нагрузку ровной.
Логирование и метрики
ограничение скорости отправки логов/ивентов в центральный storage: всплески складываются в «ведро», но отправка идёт с постоянным rate, лишнее может отбрасываться.
Если формулировка требований звучит как «равномерный поток запросов, пиков практически нет, а если есть, то допускается небольшое сглаживание», это обычно про Leaky Bucket (в отличие от Token Bucket, который допускает гибкое управление всплесками запросов).
Локальные и распределённые реализации
Алгоритмы ограничения могут применяться как локально (в рамках одного узла), так и распределённо (с учётом состояния кластера). Локальные реализации часто используются в библиотеках или middleware: например, в Java-библиотеке Bucket4j реализован токен-бакет [16], а в Resilience4j – фиксированное окно (счётчики сбрасываются по циклам) [42]. В NGINX и Envoy Rate Limiting обычно применяют модификации вышеперечисленных алгоритмов (NGINX – протекающее ведро [35] с очередью запросов, Envoy поддерживает токен-бакет и гибкие стратегии).
Для распределённых систем обычно используют централизованные хранилища состояния. Так, Redis часто применяется для реализации лимитеров: с помощью атомарных команд INCR/EXPIRE реализуется фиксированное окно [41], с Lua-скриптами – токен-бакет или скользящее окно. Cloudflare и другие крупные сервисы использовали адаптированные алгоритмы: например, они разделили счётчики запросов на шардированные хранилища и добавили усреднение по предыдущему окну (sliding window) [14] [16]. Интеграция через API Gateway или сервисную сетку (Service Mesh) позволяет распределять правила лимитирования по множеству узлов.
Примеры:
NGINX встроенно поддерживает протекающее ведро (с буфером запросов) [35] и позволяет настраивать параметры limit_req и burst.
Envoy Proxy может использовать политику RateLimit (где в основе – токен-бакет или простое ограничение числа запросов).
Redis часто применяется для распределённых лимитеров: например, минимальные реализации фиксированного окна или токен-бакета с разделением по ключам и срокам жизни.
Cloudflare в облачном WAF применяет rate limiting с гибридом алгоритмов; в одном из случаев они описали использование скользящего окна с частичным учётом предыдущего периода для более точной фильтрации атак [14].
Java-библиотеки: Bucket4j реализует токен-бакет (с возможностью работы поверх Redis/JCache для распределённых лимитов) [16]. Resilience4j RateLimiter использует концепцию фиксированного окна со скользящим счётом разрешений [42].

Сравнительная таблица алгоритмов
Критерий
Fixed Window
Sliding Window
Token Bucket
Leaky Bucket
Точность контроля
Низкая – допускает скачки на границах окон
Высокая – сглаживает границы, учитывает историю запросов
Высокая – стабилизирует скорость, динамически накапливает токены
Средняя – выходной поток стабилен, но не гибок. Не подходит для больших всплесков.
Гибкость / адаптивность
Низкая – порог фиксирован, не реагирует на изменение профиля
Средняя – точность высокая, но без буферизации (ограничения в реальном времени)
Высокая – накапливает токены при простое, допускает кратковременные пики
Низкая – не допускает пиков (очередь обрабатывается с постоянной скоростью)
Обработка пиковой нагрузки
Плохая: имеет провал на границах окон, временно может пропустить два пика подряд
Лучше, чем fixed: окно скользит и учитывает часть предыдущего периода
Отлично для кратких всплесков: токен-бакет аккумулирует запасы
Сглаживает пики до заданного уровня, но часть пиковых запросов может быть потеряна при переполнении
Накладные расходы
Низкие – один счётчик (О(1) памяти/CPU)
Высокие – хранение истории запросов или нескольких счётчиков (О(N) памяти)
Средние – хранит счёт токенов и синхронизацию пополнения (может требовать таймеры или луа-скрипты)
Средние – буфер запросов (очередь) плюс вычисление скорости утечки (таймер)
Сложность реализации
Низкая – очень просто (подсчёт в окне)
Высокая – поддержка списков/логов или множественных окон
Средняя/высокая – нужно реализовать пополнение токенов (таймер или атомарные операции)
Средняя – очередь и расчёт утечки требуют синхронизации, но логика понятна
Распределённая реализация
Простая – счётчики можно хранить в Redis/базе с TTL
Сложная – требует согласованного ведения временных меток на всех узлах
Средняя – возможно через централизованное хранилище токенов (Redis, consistent hashing)
Сложая – общий буфер сложно разделить, чаще локальное решение (как в NGINX)

Таким образом, выбор алгоритма зависит от требований системы: Fixed Window прост и эффективен при равномерном трафике, но плохо защищает от пиков. Sliding Window более точен, но ресурсоёмок и сложен. Token Bucket хорошо справляется с кратковременными всплесками, сочетая гибкость и точность, но требует более сложной синхронизации. Leaky Bucket обеспечивает плавный выход запросов из системы, нивелируя пики, но это достигается ценой задержек и отказа части запросов при перегрузках. Каждый алгоритм имеет свои преимущества и ограничения [8] [41].

2.2. Архитектурные модели интеграции механизмов ограничения в распределённые системы

Ограничение частоты запросов (rate limiting) в распределённых системах может реализовываться разными алгоритмами. Например, фиксированное окно (Fixed Window) просто считает запросы в жёстко определённый временной интервал. У этого подхода есть «пиковая» проблема, так как возле границы окна может пройти в два раза больше запросов (из текущего и следующего окна). Другой вариант – скользящее окно (Sliding Window), которое смягчает эффект границы, но требует хранения всех меток запросов (что дороже по памяти и вычислениям) [28]. Наиболее гибким часто считается алгоритм «токеновое ведро» (Token Bucket) – токены накапливаются в ведре с заданной скоростью, а каждый запрос «тратит» один токен, что естественно позволяет обрабатывать ограниченные всплески при стабильном долгосрочном лимите. В реальных системах иногда используются гибридные схемы (например, скользящее окно-счётчик), но выбор алгоритма влияет на устойчивость и пропускную способность: одни алгоритмы проще реализовать, другие дают точное ограничение ценой сложности.

Встраивание в код приложения (middleware)
Самый простой вариант – встроить лимитер прямо в сервис или middleware-приложение. Например, в проектах с Spring Boot популярны библиотеки Bucket4j или Resilience4j. С помощью Spring-фильтра или аспектов можно перехватывать входящие запросы и проверять счётчик по клиенту (IP или API-ключу). В Bucket4j создаётся ведро (с конфигурацией, скажем, 200 токенов в минуту) и ProxyManager, который через Redis обеспечивает синхронизацию этого ведра между инстансами. Для каждого запроса фильтр вызывает bucket.tryConsume(); при нехватке токенов возвращается HTTP 429 [26]. Аналогично, Resilience4j позволяет аннотировать методы @RateLimiter, настраивая в properties параметры limit-for-period и refresh-period [13]. Такие решения очень гибкие и не требуют внешних сервисов, минимизируя задержки (всё происходит в памяти JVM). Однако у них есть недостаток: каждый экземпляр сервиса считает лимиты локально, поэтому при масштабировании нагрузки на несколько узлов совокупное число разрешённых запросов увеличивается пропорционально числу инстансов [21]. Чтобы избежать этого, нужно либо жертвовать «глобальностью» лимита, либо вводить централизованное хранилище.
Встроенный подход даёт хорошую контекстную адаптацию (лимитер может учитывать внутренние метрики, бизнес-логику и т.д.), но усложняет управление в больших кластерах: конфигурацию нужно дублировать по всем сервисам, и нет единой точки контроля. Среди примеров встраивания – библиотеки на других языках (например, rate-limiter-flexible для Node.js или Guava RateLimiter для JVM), хотя при распределённом деплое они также сталкиваются с проблемой согласованности.
API-шлюзы (API Gateway)
Многие организации выносят ограничение запросов на уровень API-шлюза – тем самым реализуя единый точечный контроль «на входе» системы. Например, AWS API Gateway использует алгоритм токенового ведра: для всех API в аккаунте/регионы задаются лимиты steady-state RPS и burst. При превышении AWS возвращает HTTP 429 [10]. Потребитель может настроить собственные quotas через Usage Plans (например, 100 запросов в секунду на ключ API). Azure API Management предлагает политику <rate-limit-by-key>, ограничивающую количество запросов по заданному ключу за период времени. При превышении лимита Azure тоже отдаёт 429. В документации Microsoft отмечено, что из-за распределённой природы такие лимиты работают «best-effort» и не всегда абсолютно точны [33] (из-за задержек синхронизации).
Open-source шлюзы (Kong Gateway) имеют плагины rate-limiting, которые очень гибко конфигурируются: можно задавать несколько окон и лимитов для каждого API и пользователя, а в качестве хранилища счётчиков поддерживается локальная память, Redis, Postgres или Cassandra [28]. Kong позволяет выбирать синхронную или eventual-консистентную модель обновления данных. Примерно то же предоставляет Spring Cloud Gateway: в нём есть фильтр RequestRateLimiter, который по умолчанию использует Redis и алгоритм токенового ведра. Конфигурация через свойства может задавать, например, replenishRate=500 (RPS) и burstCapacity=1000, что значит долгосрочно 500 запросов в секунду с возможностью кратковременного пика до 1000 [43]. При этом в качестве ключа (Client ID) фильтр использует principal пользователя или может применяться произвольный KeyResolver. Такие решения удобны тем, что вся политика лимитирования централизована в одном компоненте, простая в управлении. Однако они могут не видеть специфический контекст приложения (например, тип ресурса внутри микросервиса) и добавляют небольшую задержку (запрос в Redis).
Пример AWS API Gateway (token bucket, лимиты на аккаунт/стадию) [10]; Azure APIM (rate-limit-by-key policy) [33]; Kong Gateway (плагин с поддержкой Redis/Cassandra) [28]; Spring Cloud Gateway (RequestRateLimiter с Redis и настройками replenishRate/burstCapacity) [43].
Централизованный сервис ограничения запросов
Вместо интеграции в каждый сервис или gateway можно создать отдельный сервис (Rate Limiter Service), который отвечает за подсчёт и проверку лимитов. Все компоненты системы при получении запроса посылают в него асинхронный или синхронный запрос о разрешении (например, перед выполнением бизнес-логики сервис спрашивает: «можно ли ещё N запросов от этого пользователя?»). Такой подход даёт глобальное единое хранилище состояния и согласованность — в микросервисной архитектуре это упрощает соблюдение «общих» лимитов, которые должны действовать сквозь несколько сервисов [21]. К недостаткам относится повышенная задержка, ведь каждый запрос требует дополнительного хопа к лимитеру (иногда в несколько миллисекунд). Кроме того, само ограничивающее звено становится потенциальной точкой отказа. Для устойчивости его обычно разворачивают в кластере (несколько инстансов за load balancer), но всё равно нужен fallback-план на случай отказа. Популярная схема — делать локальные (in-memory) счётчики со сквозной синхронизацией с сервисом в фоне [28]. Если централизованный лимитер недоступен, система может перейти в «open» режим (разрешать все запросы, рискуя перегрузкой) или «closed» (блокировать все, вызывая деградацию), либо использовать локальный кэш последних состояний и circuit-breaker [21].
К недостаткам централизованной схемы относят и гонки при обновлении счётчиков: если просто считать запросы через read–modify–write, можно получить рассинхронизацию (несколько потоков одновременно инкрементируют «старое» значение). Чтобы избежать этого, применяют атомарные операции (INCR/EXPIRE в Redis или специализированные решения как RedisCell), или блокировки (но они плохо масштабируются) [21] [28]. В общем, центральный лимит-сервис добавляет управляемости (единый механизм), но усложняет архитектуру и немного ухудшает задержки.
Хранилище счётчиков: Redis и распределённые кэши.
Чаще всего за хранилище состояния ограничителя берут быструю In-Memory БД — прежде всего Redis или похожий кластерный кэш (Hazelcast, Ignite). В Redis обычно используют комбинацию команд INCR и EXPIRE или Lua-скрипты для атомарного обновления счётчика в рамках окна. Например, скрипт на Lua может выполнить GET+INCR+EXPIRE одним батчем, возвращая разрешение на запрос или его отказ [30]. Redis поддерживает кластерный режим с шардами (hash slots) и репликами, что позволяет масштабировать лимитеры: данные клиентов хэшируются так, чтобы все запросы от одного клиента шли к одному узлу (consistent hashing или стандартные слоты Redis) [21]. Репликация/HA Redis (Sentinel, Cluster, Redis Enterprise) обеспечивает отказоустойчивость – при падении мастера данными займётся его реплика.
Масштабирование и согласованность. Чтобы не потерять атомарность обновлений, используют лишь один ключ на данные по одному клиенту/окну и применяют атомарные команды. Если же шардировать логику, надо обеспечить, чтобы один клиент попадал на один и тот же shard (иначе будет отдельный счётчик на разных нодах). При очень больших нагрузках можно использовать специализированные реализации (например, Redis Cell или Token Bucket Servlet от Redis Labs), которые уже распределяют запросы по кластерам и обеспечивают строгое лимитирование в распределённой среде [21]. Другие реализации — хранить время запросов в Sorted Set (для скользящего окна) или вести простой счётчик со временем жизни.
Отказоустойчивость. Если Redis недоступен, система должна решать, как себя вести. Простые стратегии: разрешать все запросы (fail-open) или блокировать все (fail-close), но и то, и другое рискованно. Гораздо лучше держать локальный fallback-кэш или circuit-breaker: например, разрешить кешировать последние состояния счётчиков в памяти сервиса, а при недоступности центра отдавать решения на основе этих локальных данных [21]. Часто используют двухуровневую модель: быстрые инстансы сервиса проверяют локальный счётчик (с небольшим временным окном или предыдущим значением) и параллельно асинхронно синхронизируют результаты в Redis [28]. Такой подход минимизирует задержки и «разгоняет» ограничения, сохраняя eventual consistency.
Пример использования Redis: при реализации через Spring Cloud Gateway достаточно подключить модуль spring-boot-starter-data-redis и сконфигурировать redis-rate-limiter.replenishRate и burstCapacity [43]. В собственном коде на Spring Boot + Bucket4j создаётся LettuceBasedProxyManager и BucketConfiguration (пропускная способность и период), после чего фильтр обращается к proxyManager [26]. Для случая простого счётчика достаточно выполнить redis.eval(script, Mode.READ_WRITE, scriptCode, ReturnType.BOOLEAN, List.of(key), List.of(limit, ttl)) [30].
Компромиссы архитектуры
Каждый подход к rate limiting меняет акценты между простотой, надёжностью, задержкой и управляемостью. Встраивание в приложение даёт минимальную задержку (всё в памяти), максимальную гибкость и простоту (используются привычные библиотеки), но не обеспечивает единых глобальных лимитов в кластере; при масштабировании возможны случайные превышения суммарного лимита [21]. API-шлюзы централизуются у входа, обеспечивая согласованность на уровне всей системы и простоту администрирования (единственный конфиг для всех запросов), но они не знают деталей сервисов внутри и добавляют несколько миллисекунд на каждый запрос. Централизованный сервис даёт самый глобальный контроль и возможность сложной логики, однако – самый высокий overhead на каждый запрос и риск становиться узким местом. При его использовании часто приходится жертвовать strict consistency ради производительности (более мягкие лимиты или eventual sync) [28].
Разработчики часто смешивают подходы: реализуют грубое ограничение на краю (gateway или CDN) и фильтры в сервисах для уточнения (бизнес-логика), либо используют многоуровневый rate limiting (глобальный + локальные лимиты). Компромисс также заключается в выборе алгоритма: простой fixed window легко делать централизованным и быстрым, но он плохо переносит резкие скачки, тогда как более сложные схемы (скользящее окно, логи) точнее, но дороже в ресурсах [28].
Мониторинг, логирование и аварийное переключение
Для хорошей устойчивости системы требуется её мониторинг. В метрики входят число пришедших, дозволенных и заблокированных запросов, средние задержки проверки, процент отказов и т.д. Современные фреймворки (например, Spring Actuator с Resilience4j) предоставляют Endpoint’ы или метрики Prometheus для трассировки состояний правил лимитирования. Хорошей практикой считается возвращать клиенту стандартные заголовки лимитов (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset), чтобы потребители могли адаптировать поведение [21]. Также стоит предусмотреть дашборд для клиентов, где по API можно узнать оставшийся лимит, и логирование событий превышения (разрещены/отклонены запросы) для последующего анализа.
В случае сбоев лимитеров необходимо иметь планы аварийного переключения. Например, если Redis-пул лимитеров выходит из строя, система может на короткое время перейти в «fail-open» (разрешать все запросы, временно снимая защиту) или «fail-close» (отказывать, чтобы предотвратить перегрузку) [21]. Можно также внедрить circuit breaker перед запросами в лимитер: если он считает, что лимитер недоступен или слишком медленно отвечает, временно пропускать трафик локально, одновременно сигнализируя об ошибке мониторинговым системам. Заголовки и логи отказов (HTTP 429) должны фиксироваться в системах сбора метрик (Prometheus/Grafana, ELK) для своевременного обнаружения проблем.
Как итог, архитектура rate limiting – это баланс между простотой и комплексностью. Плюрализм моделей (локальная проверка, gateway, централизованный сервис) и комбинирование их даёт максимальную надёжность и гибкость в защите API и ресурсов системы. Важна комплексная диагностика: подробное логирование ограничений, регулярные тесты лимитеров и мониторинг реального времени. Всё это помогает поддерживать систему в надёжном состоянии и быстро реагировать на атаки или резкие нагрузки.

2.3. Компромиссы между точностью, отказоустойчивостью и адаптивностью механизмов ограничения запросов в распределённых системах

Компромиссы точности, отказоустойчивости и производительности. Проектируя распределённый лимитер запросов, приходится балансировать между точностью учёта, скоростью отклика и устойчивостью к отказам. Например, централизованное хранилище счётчиков (например, Redis) позволяет проводить атомарное обновление и гарантирует согласованность значений по всем узлам, но добавляет сетевую задержку и становится узким местом по доступности [45]. Аналогично, алгоритм с хранением всех меток запросов за окно (sliding window log) обеспечивает идеальную точность подсчёта за счёт ведения списка временных меток каждого запроса, но требует огромного объёма памяти и CPU. Гибридные решения (sliding window counter) значительно экономят ресурсы, храня лишь два счётчика (текущего и предыдущего окон), но допускают небольшую погрешность оценки числа запросов [25]. На практике статическая настройка параметров токен-бакета (размер ёмкости и скорость пополнения) затруднена из-за изменчивости реальных нагрузок и неоднородности трафика. В связи с этим в современных системах всё чаще рассматриваются подходы, при которых значения параметров лимитирования корректируются динамически на основе анализа метрик нагрузки. Для такого анализа могут использоваться внешние интеллектуальные аналитические компоненты, осуществляющие прогнозирование интенсивности запросов и выявление аномальных паттернов трафика, при этом сами алгоритмы ограничения запросов остаются детерминированными и неизменными.
Таким образом, наиболее точные схемы обычно медленнее или сложнее в масштабировании, тогда как быстрые и лёгкие решения жертвуют некоторой точностью контроля.
Проблемы согласованности в кластерах (eventual consistency, split-brain, quorum). В распределённой среде разные узлы могут иметь рассогласованные представления об остатке квоты. Часто системы сознательно выбирают eventual consistency: небольшая задержка распространения обновлённых счётчиков считается допустимой ценой за высокую доступность. Так, на ресурсе HelloInterview по проектированию лимитеров подчёркивается, что система должна быть высокодоступной, а небольшие задержки в применении лимитов между узлами допустимы [25]. Это означает, что во время сетевого разделения (split-brain) каждый фрагмент кластера может обслуживать запросы, но видеть лишь часть общей нагрузки. При классическом сценарии split-brain два разделённых кластера могут независимо разрешать клиенту запросы, и в результате общее число обработанных запросов превысит лимит. В качестве примера техники компенсации: Redis Cluster, настроенный с репликами, использует асинхронную репликацию, что соответствует выбору в пользу доступности над строгой согласованностью. Как отмечено в документации, Redis Cluster обычно применяет асинхронную репликацию, что вписывается в концепцию доступности в ущерб согласованности (eventual consistency допустима даже с небольшими неточностями в крайних случаях) [29]. Альтернативно можно применять консенсусные алгоритмы с кворумом для записи/чтения, но это добавляет задержки и снижает отказоустойчивость: при разрыве сети часть операций будет блокироваться, чтобы не нарушить целостность данных.
Конфликты при репликации счётчиков и поведение при перегрузках. Распределённая репликация счётчиков порождает дополнительные сложности. При интенсивной параллельной обработке запросов может возникать состояние гонки: если два узла одновременно инкрементируют один счётчик без синхронизации, итоговое значение может оказаться заниженным. Для предотвращения этого используют атомарные операции или механизмы блокировок [45]. Например, в Redis команды INCR/DECR выполняются атомарно, а при более сложных алгоритмах применяют LUA-скрипты или CAS-проверки. Однако при использовании кэширования или отложенной репликации точность может страдать. Так, при использовании Cassandra или аналогичных leaderless хранилищ кэширующую стратегию cache-aside пишут данные и в БД, и в кэш. Если запись в базу прошла успешно, а обновление кэша – нет (например, из-за сбоя), то клиент может получить устаревший счётчик. Это подчёркивается на ресурсе SystemDesign.one, где говорится, что вероятность расхождения счётчика выше, когда запись в БД успешна, а обновление кэша терпит неудачу [46]. В условиях перегрузок возникает эффект «бега стад» (thundering herd): множество клиентов одновременно достигают лимита и пытаются повторить запрос синхронно, что создаёт резкий всплеск нагрузки. Для его смягчения рекомендуют вводить случайные задержки (jitter), делать постепенное увеличение интервалов при повторных попытках и использовать алгоритмы, сглаживающие поток (token bucket) [21].
CAP-теорема и стратегии лимитирования. По CAP-теореме, при сетевых разделениях распределённая система может обеспечить либо согласованность, либо доступность (Partition Tolerance требуется в любом случае). В контексте ограничений запросов большинство решений жертвуют согласованностью в пользу доступности: лучше кратковременно разрешить немного превышений, чем приостановить обслуживание клиентов. Как подчёркивается на  ресурсе Medium об обзоре проектирования, «для лимитера приоритет отдаётся доступности, и eventual consistency приемлема, даже если новые правила немного запаздывают» [29]. Использование внешних аналитических компонентов позволяет частично компенсировать последствия такого выбора, прогнозируя приближение пиковых нагрузок и инициируя упреждающую корректировку лимитов до наступления критических состояний. Это снижает вероятность резких перегрузок при сохранении стратегии высокой доступности и допускаемой eventual consistency. Стратегии же fail-closed или строгая запись по кворуму стремятся к консистентности (C), блокируя операции при сомнительных состояниях, но тогда теряется часть доступности. Из этого следует, что при выборе конфигурации следует решать, какой компонент CAP наиболее критичен: выбирая высокая доступность и терпимость к разбивкам (Partition Tolerant) допускает небольшие рассогласования, тогда как выбор в пользу согласованности означает более жёсткую логику блокировок и риска отказа при разделениях.
Подходы к снижению последствий отказов и неточностей. Чтобы уменьшить отрицательное влияние ошибок в механизме ограничения запросов, применяют несколько приёмов. При недоступности кэша или службы лимитов возможны варианты поведения (позволять все запросы или блокировать все). Обычно вводят fallback-стратегии: например, ведут локальный кэш последних разрешений и отказов, чтобы при недоступности основного сервиса использовать его для принятия решения. В схемах graceful degradation при сбое централизованного хранилища каждый узел начинает работать по собственным (более консервативным) ограничениям. Для повышения отказоустойчивости кластеры Redis разворачивают с репликами и механизмом выбора лидера (Sentinel/Cluster): при падении ведущего узла система автоматически переключается на резерв. Также активно применяются circuit breaker и отложенные повторы: при обнаружении высоких задержек или ошибок соединения дальнейшие обращения к хранилищу лимитов временно приостанавливаются, используя более простую логику (либо позволяя часть трафика по умолчанию). Наконец, в алгоритмы добавляют защитные механизмы при перегрузках: экспоненциальное увеличение времени ожидания перед повторами и введение случайного «дребезга» при обнулении окон [21] [45].
Практические примеры. Реальные системы воплощают эти идеи по-разному. Подобные решения опираются на автоматизированный анализ телеметрии и поведенческих характеристик трафика, что сближает их с интеллектуальными подходами к управлению нагрузкой, несмотря на сохранение классических алгоритмов лимитирования в основе. Так, сервис DDoS-Guard реализует адаптивное ограничение: он динамически рассчитывает лимиты на основе текущей загрузки сервисов (CPU/память, время обработки запросов, нагрузка БД) [29] и при достижении критических порогов постепенно фильтрует трафик от пользователей, делающих большое количество запросов. Это позволяет не применять жёсткие фиксированные квоты и выгодно отличает решение от типовых статических лимитеров. В коммерческих API и облачных платформах ограничения задокументированы жёстко. Например, глобальный REST API Cloudflare ограничивает клиента 1200 запросами за 5 минут (на токен/пользователя) [15], а также 200 запросами в секунду с одного IP. При превышении возвращается HTTP 429 с информационными заголовками о лимите. Аналогично, у публичного API GitHub аутентифицированному клиенту разрешено 5000 запросов в час, а для анонимных – лишь 60 запросов/час [19]. Крупные облачные провайдеры (AWS, GCP, Azure) также вводят квоты на вызовы API (например, через механизмы API Gateway или WAF) и реализуют защиту от перегрузок встроенными rate limiting-правилами.
Стратегии при сбоях: fail-open или fail-closed. Приоритеты архитектурных стратегий проявляются и в выборе поведения на случай отказа лимитера. Как поясняют авторы системного дизайна, fail-open означает, что при сбое лимитера все запросы разрешаются, что обеспечивает максимальную доступность, но допускает возможные перерасходы квот. Fail-closed говорит о том, что при сбое лимитера все запросы блокируются. Это защищает систему от перегрузки, но может привести к отказу среднестатистических клиентов. На практике большинство систем склоняются к fail-open для обычных операций (чтобы не выключать сервис из-за внутренних сбоев), а строгий fail-closed применяют лишь для критичных операций (например, для входа или платёжных транзакций) [45]. Выбор этой стратегии напрямую отражает компромисс в терминах CAP: fail-open жертвует согласованностью счетчиков ради сохранения доступности, а fail-closed — наоборот, пожертвует доступностью ради строгого соблюдения ограничений.
Любые алгоритмы распределённого лимитирования запросов вносят баланс между тремя факторами: точностью подсчёта, скоростью обработки и надёжностью. С одной стороны, стремление к строгой консистентности (и наивысшей точности лимитов) требует дополнительной синхронизации и добавляет задержки, а в условиях разделений сети ведёт к потере доступности. С другой – упрощение согласованности (eventual consistency, локальные оценки) снижает задержки и повышает устойчивость, но может допустить ошибочные разрешения или задержки в применении ограничений. Практические системы минимизируют негатив, комбинируя подходы: быстрые локальные проверки с последующей глобальной сверкой счётчиков, отказоустойчивые кластеры хранилищ, адаптивное регулирование лимитов и резервные механизмы при сбоях [21] [45]. При выборе архитектуры важно ясно оценить, какой компонент CAP-теоремы (согласованность или доступность) критичнее для задачи, и настроить стратегию (fail-open или fail-closed, синхронную или асинхронную репликацию) с учётом приемлемой степени неточности лимитирования.

Глава 3. Экспериментальное исследование алгоритмов ограничения запросов в распределённой системе

3.1. Методика проведения эксперимента и описание архитектуры прототипа системы

В данном разделе описываются методика проведения эксперимента и архитектура прототипа распределённой системы, разработанного для исследования влияния алгоритмов ограничения частоты запросов на устойчивость и защищённость системы. Эксперимент нацелен на оценку того, как различные стратегии лимитирования (ограничители с фиксированным окном, скользящим окном и «ведро с токенами») влияют на стабильность работы сервиса при разных сценариях нагрузки, и в особенности – как подключение внешнего интеллектуального компонента для адаптивной настройки параметров этих алгоритмов может повысить устойчивость системы.
Цель эксперимента: Оценить влияние различных алгоритмов ограничения частоты запросов на работоспособность распределённой системы при разнообразных профилях нагрузки, включая нормальные и аномальные сценарии, а также проверить, способен ли внешний интеллектуальный модуль (на основе моделей прогнозирования) адаптивно управлять параметрами лимитирования для повышения устойчивости системы. Иными словами, эксперимент сравнивает статическое лимитирование запросов с адаптивным подходом, где параметры алгоритма динамически подстраиваются на основе предсказаний о входящем трафике.
Архитектура прототипа системы: Тестовая платформа реализована в виде нескольких взаимодействующих сервисов, развернутых в Docker-контейнерах и коммуникационно связанных между собой. Основу прототипа составляют три микросервиса, написанные на Java (Spring Boot):
Сервис A (клиент-генератор нагрузки): Имитирует множество клиентов и генерирует HTTP-запросы заданной интенсивности и профиля. Этот сервис позволяет воспроизводить разные типы нагрузочных сценариев (равномерный поток, всплески, аномальное поведение и т.д.) путем настройки частоты и паттерна отправки запросов.
Сервис B (целевой сервис): Представляет собой защищаемое приложение – условно «рабочий» сервис, к которому направляются запросы. В прототипе он играет роль упрощенной бизнес-логики (например, возвращает фиктивный ответ), основная задача – подвергнуться нагрузке и предоставлять метрики производительности (время отклика, процент успешных/ошибочных запросов). Сервис B служит индикатором того, как лимитирование защищает приложение: его стабильность под нагрузкой будет оцениваться.
Сервис C (балансировщик с механизмом лимитирования): Является промежуточным прокси между A и B. Он принимает входящие запросы от генератора (A) и решает, пропустить запрос к сервису B или отклонить его, исходя из политики ограничения частоты. Сервис C реализует непосредственно три алгоритма лимитирования: фиксированное окно, скользящее окно и бакет токенов. Выбор алгоритма и его параметры (например, размер окна или емкость/скорость пополнения бакета) могут переключаться в ходе экспериментов. Сервис C функционирует как распределённый ограничитель нагрузки: для хранения счетчиков запросов и токенов используется внешнее хранилище Redis (база данных типа «ключ-значение» в памяти). Выбор Redis обусловлен необходимостью разделять состояние лимитирования между несколькими экземплярами сервисов и сохранять его за пределами памяти приложения (для поддержания консистентности при масштабировании и перезапусках). Redis развернут в отдельном контейнере; при этом предусмотрен режим отказоустойчивости: в случае сбоя связи с Redis ограничитель переводится в «открытое» состояние и перестаёт блокировать запросы, чтобы внезапно не нарушить доступность системы. Такая политика («fail-open») соответствует рекомендациям по проектированию отказоустойчивых лимитеров частоты запросов.
Помимо основных сервисов, в прототип включены инструменты мониторинга и интеллектуальный модуль:
Система мониторинга (Prometheus): Каждый из сервисов A, B, C экспонирует метрики (через Spring Boot Actuator), такие как время отклика запросов, количество обработанных и отклонённых запросов, состояние очередей и пр. Эти данные собираются системой мониторинга Prometheus, что позволяет отслеживать поведение системы в реальном времени и впоследствии анализировать результаты каждого эксперимента. Данные метрик используются для оценки критериев эффективности (описаны ниже).
Интеллектуальный компонент: В качестве внешнего аналитического модуля разработан отдельный сервис на Python, интегрированный с основными компонентами системы. Его задача – осуществлять краткосрочный прогноз входящего трафика и выдавать рекомендации по настройке параметров алгоритма лимитирования в режиме реального времени. Этот сервис использует модель прогнозирования временных рядов Prophet (разработанная Facebook) для анализа истории поступления запросов и предсказания их интенсивности в ближайшем будущем. На основе прогноза модуль генерирует рекомендации, например: увеличить лимит запросов в окне, если ожидается рост легитимной нагрузки, или наоборот, снизить порог/уменьшить размер окна при прогнозе потенциального всплеска (что позволит строже ограничивать поступающий поток). Коммуникация между компонентом и сервисом C реализована через REST API: сервис C периодически запрашивает у интеллектуального модуля обновлённые настройки или прогноз. Таким образом, политика ограничения становится адаптивной: параметры алгоритма могут изменяться на лету под воздействием внешнего «советчика». Подход с упреждающим адаптивным управлением вдохновлён практикой прогнозного масштабирования (predictive autoscaling) – когда использование моделей (Prophet, ARIMA и др.) позволяет заранее подготовить систему к росту нагрузки. Здесь вместо масштабирования серверов используется аналогичный принцип для упреждающей настройки лимитирования запросов.
Сценарии нагрузки эксперимента: Для оценки работы алгоритмов выбраны несколько характерных сценариев, покрывающих как штатные, так и экстремальные режимы:
Равномерная нормальная нагрузка. Сервис A генерирует стабильный поток запросов с более-менее постоянной скоростью, не превышающей заранее установленный лимит. Этот сценарий имитирует обычное поведение пользователей при равномерном входящем трафике. Он служит для базовой проверки корректности работы ограничителей в нормальных условиях: система не должна отклонять запросы без необходимости, время отклика должно оставаться низким.
Кратковременные всплески трафика. Нагрузочный профиль включает периоды резкого увеличения частоты запросов (например, краткие пики, превышающие лимит в 2–3 раза), чередуемые с нормальными интервалами. Такой сценарий моделирует ситуации типа флешмоба пользователей или кратковременного скачка активности. Цель – проверить, как разные алгоритмы справляются с короткими перегрузками: фиксированное окно может пропускать burst-трафик на границах окна, скользящее окно должно сглаживать выбросы, а бакет токенов – допускать ограниченные всплески за счёт накопленных токенов. Также анализируется, помогает ли интеллектуальный компонент предвидеть всплеск (по ранним признакам роста трафика) и заранее скорректировать лимиты, уменьшая очередь запросов или временно увеличивая пропускную способность.
Превышение порога запросов и аномальное поведение. В этом сценарии имитируется чрезмерная или некорректная нагрузка: например, один или несколько условных клиентов пытаются отправлять запросы с чрезмерно высокой частотой (значительно превышая лимиты) или генерируют нехарактерные паттерны трафика. Такой тест подобен ситуации DDoS-атаки либо эксплуатации API недобросовестными пользователями. Ожидается, что алгоритмы лимитирования эффективно отсеют избыточные запросы, защитив сервис B. Будет измеряться, какой процент вредоносного трафика отклонён каждым алгоритмом и не вызывает ли интенсивное лимитирование побочных эффектов (например, деградации производительности C). Адаптивный модуль в этом случае мог бы обнаружить аномалию (резкий рост запросов) и ужесточить лимиты автоматически.
Сбой хранилища или деградация узла. Дополнительно вводится отказоустойчивый сценарий: намеренное отключение или замедление Redis во время теста, а также возможное снижение производительности одного из сервисов. Это позволяет проверить устойчивость всей системы в условиях частичного сбоя. В частности, отключение Redis проверяет поведение механизма лимитирования в отсутствии общего хранилища: благодаря реализации режима «открытого положения при отказе» система должна временно перестать ограничивать запросы, но при этом сервис B должен оставаться доступным. Анализируется, приводит ли такой сбой к заметному росту времени отклика или к потерям данных, и как быстро система восстанавливается, когда Redis снова становится доступен.


Метрики оценки: В ходе эксперимента для каждой комбинации сценария нагрузки и алгоритма (с адаптацией или без) собираются следующие ключевые метрики:
Среднее время отклика – показатель производительности, вычисляемый по ответам сервиса B. Отражает, насколько быстро система обрабатывает запросы в данных условиях. Рост времени отклика под нагрузкой сигнализирует о приближении к пределу пропускной способности или о перегрузке.
Процент отклонённых запросов – доля запросов, которые были заблокированы ограничителем (HTTP-ответы с кодом 429 Too Many Requests), от общего числа запросов. Этот показатель характеризует «жёсткость» применяемого ограничения: высокий процент означает, что много запросов отсекается для защиты системы. В нормальных условиях (сценарий 1) этот процент должен быть близок к нулю, а в условиях всплесков или атак (сценарии 2 и 3) ожидается рост. Сравнение этого показателя между алгоритмами покажет их относительную строгость и способность фильтровать нагрузку.
Устойчивость при пиковых нагрузках – комплексная метрика, оценивающая способность системы сохранять работоспособность на пике нагрузки. Она выражается в отсутствии отказов сервиса B (т.е. отсутствии падений или критических ошибок) и в быстроте восстановления нормального времени отклика после прохождения пика. Также сюда можно включить максимальное отклонение времени ответа на пике и степень деградации throughput (числа успешно обработанных запросов в секунду) во время перегрузки.
Уровень защиты от перегрузок – оценивается качественно, исходя из сочетания вышеуказанных метрик, а также логов системы. Данный критерий отражает, насколько эффективно прототип предотвращает негативные последствия перегрузки. Например, если при сценарии 3 целевой сервис B продолжает функционировать стабильно, а большая часть избыточных запросов отфильтрована механизмом C, можно говорить о высоком уровне защиты. Напротив, если какой-то алгоритм пропускает слишком много лишних запросов и это приводит к деградации B, уровень защиты считается низким.
Ограничения исследования: Следует подчеркнуть ряд упрощений и допущений, принятых в эксперименте. Во-первых, модель Prophet в интеллектуальном компоненте не проходила отдельного процесса обучения на исторических данных в рамках работы – предполагается использование модели с базовой настройкой или обученной на некой типовой выборке. Таким образом, качество прогноза как таковое не анализируется и не гарантируется. Интеллектуальный модуль рассматривается абстрактно как внешний источник рекомендаций (в духе «oracle»), и основное внимание уделяется не точности предсказания, а тому, как сам факт наличия прогнозирующего блока влияет на работу системы (например, не приводит ли задержка на вычисление прогноза к ухудшению отклика, насколько оперативно система реагирует на рекомендации и т.д.). Во-вторых, эксперимент не сравнивает разные модели машинного обучения между собой и не оптимизирует алгоритм Prophet – сфера искусственного интеллекта затрагивается лишь постольку, поскольку демонстрируется возможность его интеграции в систему управления нагрузкой. Наконец, следует отметить, что выводы по устойчивости и защищённости системы делаются в контексте контролируемого прототипа и заданных сценариев. Реальные системы могут иметь дополнительные факторы (например, более сложную бизнес-логику в сервисе B, сетевые задержки, многоузловую топологию), которые не полностью воспроизведены в нашей экспериментальной установке. Тем не менее, полученные результаты позволяют выявить общие тенденции и эффекты от применения разных алгоритмов ограничения запросов и адаптивной настройки их параметров.




3.2. Результаты эксперимента и сравнительный анализ алгоритмов лимитирования




Заключение



Список использованных источников

1. Статьи в периодических изданиях и сборниках статей

Chen X., Li Y., Zhang H. Adaptive rate limiting for cloud-native microservices // Journal of Cloud Computing. – 2022. – Vol. 11, No. 1. – P. 1–18.
Kaur R., Singh P. Mitigating cascading failures in distributed systems under high load // IEEE Access. – 2023. – Vol. 11. – P. 45678–45692.
Zhang L., Wang S. Load-aware throttling mechanisms for resilient distributed architectures // Future Generation Computer Systems. – 2024. – Vol. 148. – P. 112–124.

2. Книги, монографии

Kleppmann M. Designing Data-Intensive Applications. – Sebastopol: O’Reilly Media, 2022. – 616 p.
Newman S. Building Microservices. – 2nd ed. – Sebastopol: O’Reilly Media, 2021. – 582 p.
Richards M., Ford N. Fundamentals of Software Architecture. – Sebastopol: O’Reilly Media, 2023. – 448 p.

3. Электронные ресурсы

A10 Networks. 5 most famous DDoS attacks [Электронный ресурс]. – URL: https://www.a10networks.com/blog/5-most-famous-ddos-attacks/ (дата обращения: 14.09.2025).
Alibaba Cloud. Rate limiting and degradation strategies [Электронный ресурс]. – URL: https://www.alibabacloud.com/blog/three-strategies-of-high-concurrency-architecture-design---part-2-rate-limiting-and-degradation_601162 (дата обращения: 16.11.2025).
AppMaster. Модели отказоустойчивости архитектуры микросервисов [Электронный ресурс]. – URL: https://appmaster.io/ru/blog/modeli-otkazoustoichivosti-arkhitektury-mikroservisov (дата обращения: 08.10.2025).
AWS. API Gateway request throttling [Электронный ресурс]. – URL: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html (дата обращения: 10.12.2025).
AWS. Disaster recovery and resiliency for API Gateway [Электронный ресурс]. – URL: https://docs.aws.amazon.com/apigateway/latest/developerguide/disaster-recovery-resiliency.html (дата обращения: 17.10.2025).
AWS. Mitigate interaction failure by throttling requests [Электронный ресурс]. – URL: https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_throttle_requests.html (дата обращения: 02.10.2025).
Baeldung. Spring Boot with Resilience4j [Электронный ресурс]. – URL: https://www.baeldung.com/spring-boot-resilience4j (дата обращения: 04.12.2025).
Cloudflare. Counting things at scale [Электронный ресурс]. – URL: https://blog.cloudflare.com/counting-things-a-lot-of-different-things (дата обращения: 25.11.2025).
Cloudflare Developers. API limits reference [Электронный ресурс]. – URL: https://developers.cloudflare.com/fundamentals/api/reference/limits (дата обращения: 07.01.2026).
DreamFactory. Distributed rate limiting with open source tools [Электронный ресурс]. – URL: https://blog.dreamfactory.com/how-distributed-rate-limiting-works-with-open-source-tools (дата обращения: 10.11.2025).
GeeksforGeeks. Rate limiting algorithms – system design [Электронный ресурс]. – URL: https://www.geeksforgeeks.org/system-design/rate-limiting-algorithms-system-design (дата обращения: 13.11.2025).
GeeksforGeeks. Vulnerabilities and threats in distributed systems [Электронный ресурс]. – URL: https://www.geeksforgeeks.org/computer-networks/vulnerabilities-and-threats-in-distributed-systems/ (дата обращения: 23.09.2025).
GitHub. Rate limits for the REST API [Электронный ресурс]. – URL: https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api (дата обращения: 11.01.2026).
GitHub. Romanow / Gateway lecture [Электронный ресурс]. – URL: https://github.com/Romanow/gateway-lecture (дата обращения: 11.10.2025).
Gravitee. Rate limiting APIs at scale [Электронный ресурс]. – URL: https://www.gravitee.io/blog/rate-limiting-apis-scale-patterns-strategies (дата обращения: 07.12.2025).
Habr. Инженерная надежность и отказоустойчивость распределенной системы [Электронный ресурс]. – URL: https://habr.com/ru/companies/otus/articles/561630 (дата обращения: 05.09.2025).
Habr. Практики устойчивости высоконагруженных сервисов [Электронный ресурс]. – URL: https://habr.com/ru/companies/vk/articles/945292 (дата обращения: 29.09.2025).
Heaad. Информационная безопасность [Электронный ресурс]. – URL: https://heaad.ru/information-security (дата обращения: 08.09.2025).
HelloInterview. Distributed rate limiter [Электронный ресурс]. – URL: https://www.hellointerview.com/learn/system-design/problem-breakdowns/distributed-rate-limiter (дата обращения: 22.12.2025).
INNOQ. Distributed rate limiting with Spring Boot and Redis [Электронный ресурс]. – URL: https://www.innoq.com/en/blog/2024/03/distributed-rate-limiting-with-spring-boot-and-redis (дата обращения: 01.12.2025).
KongHQ. 10 ways microservices create new security challenges [Электронный ресурс]. – URL: https://konghq.com/blog/engineering/10-ways-microservices-create-new-security-challenges (дата обращения: 20.09.2025).
KongHQ. Designing a scalable rate limiting algorithm [Электронный ресурс]. – URL: https://konghq.com/blog/engineering/how-to-design-a-scalable-rate-limiting-algorithm (дата обращения: 28.11.2025).
Medium. Rate limiter system design [Электронный ресурс]. – URL: https://medium.com/@lakshyachampion/rate-limiter-system-design-c678f166e94b (дата обращения: 25.12.2025).
Medium. Rate limiting in Redis using Lua [Электронный ресурс]. – URL: https://medium.com/@ramachandrankrish/rate-limiting-in-redis-using-lua-script-61774e74e270 (дата обращения: 16.12.2025).
Medium. Yukelson D. Overview [Электронный ресурс]. – URL: https://medium.com/@dmitry.yukelson/overview-c2c40f2e300f (дата обращения: 03.01.2026).
Microsoft Azure. Reliability – API Management [Электронный ресурс]. – URL: https://learn.microsoft.com/en-us/azure/reliability/reliability-api-management (дата обращения: 20.10.2025).
Microsoft Azure. Api management – Rate limit by key policy [Электронный ресурс]. – URL: https://learn.microsoft.com/en-us/azure/api-management/rate-limit-by-key-policy (дата обращения: 10.12.2025).
Mofidi. Application layer DDoS attacks and mitigation [Электронный ресурс]. – URL: https://www.mofidi.us/ru/application-layer-ddos-attacks-and-how-mitigated.html (дата обращения: 05.10.2025).
NGINX. Rate limiting with NGINX [Электронный ресурс]. – URL: https://blog.nginx.org/blog/rate-limiting-nginx (дата обращения: 19.11.2025).
OAuth 2.0 Authorization Framework [Электронный ресурс]. – URL: https://oauth.net/2 (дата обращения: 29.10.2025).
OWASP. API Security Top 10 – 2023 [Электронный ресурс]. – URL: https://owasp.org/API-Security/editions/2023/en/0x11-t10 (дата обращения: 23.10.2025).
PlantagoWeb. Создание защищённых API с rate limiting [Электронный ресурс]. – URL: https://plantagoweb.ru/blog/sozdanie-zashchishchyonnykh-api-s-rate-limiting-cors-i-bezopasnym-cors-proksi-934221/ (дата обращения: 26.09.2025).
Pragmatic Engineer. Resiliency in distributed systems [Электронный ресурс]. – URL: https://blog.pragmaticengineer.com/resiliency-in-distributed-systems (дата обращения: 26.10.2025).
RapidAPI. Keys and key rotation [Электронный ресурс]. – URL: https://docs.rapidapi.com/docs/keys-and-key-rotation (дата обращения: 04.11.2025).
Redis. Rate limiting glossary [Электронный ресурс]. – URL: https://redis.io/glossary/rate-limiting (дата обращения: 07.11.2025).
Resilience4j. RateLimiter documentation [Электронный ресурс]. – URL: https://resilience4j.readme.io/docs/ratelimiter (дата обращения: 22.11.2025).
Spring.io. API rate limiting with Spring Cloud Gateway [Электронный ресурс]. – URL: https://spring.io/blog/2021/04/05/api-rate-limiting-with-spring-cloud-gateway (дата обращения: 13.12.2025).
Spirzen. Микросервисы и интеграция [Электронный ресурс]. – URL: https://spirzen.ru/encyclopedia/Инфраструктура%20и%20безопасность/7.05.%20Микросервисы%20и%20интеграция/122 (дата обращения: 14.10.2025).
System Design Handbook. Design a rate limiter [Электронный ресурс]. – URL: https://www.systemdesignhandbook.com/guides/design-a-rate-limiter (дата обращения: 19.12.2025).
SystemDesign.one. Distributed counter system design [Электронный ресурс]. – URL: https://systemdesign.one/distributed-counter-system-design (дата обращения: 28.12.2025).
WheelHouse IT. The AWS outage of October 2025 [Электронный ресурс]. – URL: https://www.wheelhouseit.com/the-aws-outage-of-october-2025/ (дата обращения: 17.09.2025).
World Wide Technology. API security visibility [Электронный ресурс]. – URL: https://www.wwt.com/briefing/api-security-visibility-into-an-expanding-attack-surface (дата обращения: 01.11.2025).
AWS (Google SRE). Addressing cascading failures [Электронный ресурс]. – URL: https://sre.google/sre-book/addressing-cascading-failures/ (дата обращения: 11.09.2025).

4. Нормативные документы

ISO/IEC 27001:2022 Information security management systems – Requirements.
NIST SP 800-53 Rev. 5 Security and Privacy Controls for Information Systems and Organizations. – 2023.

